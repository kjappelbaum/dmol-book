
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<meta content="Deep Learning for Molecules &amp; Materials Book" lang="en" name="description" xml:lang="en" />
<meta content="en_US" property="og:locale" />
<meta content="summary" name="twitter:card" />
<meta content="Deep Learning for Molecules &amp; Materials Book" name="twitter:description" />
<meta content="dmol.pub 📖" name="twitter:title" />
<meta content="https://dmol.pub/_static/logo.png" name="twitter:image" />
<meta content="&#64;andrewwhite01" name="twitter:site" />

    <title>8. Graph Neural Networks &#8212; deep learning for molecules &amp; materials</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/a11y.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/custom.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://dmol.pub/dl/gnn.html" />
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9. Input Data &amp; Equivariances" href="data.html" />
    <link rel="prev" title="7. Standard Layers" href="layers.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">deep learning for molecules & materials</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Overview
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  A. Math Review
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../math/tensors-and-shapes.html">
   1. Tensors and Shapes
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  B. Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/introduction.html">
   2. Introduction to Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/regression.html">
   3. Regression &amp; Model Assessment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/classification.html">
   4. Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/kernel.html">
   5. Kernel Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  C. Deep Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   6. Deep Learning Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="layers.html">
   7. Standard Layers
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   8. Graph Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data.html">
   9. Input Data &amp; Equivariances
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Equivariant.html">
   10. Equivariant Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="xai.html">
   11. Explaining Predictions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="attention.html">
   12. Attention Layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NLP.html">
   13. Deep Learning on Sequences
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="VAE.html">
   14. Variational Autoencoder
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="flows.html">
   15. Normalizing Flows
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  D. Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../applied/QM9.html">
   16. Predicting DFT Energies with GNNs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../applied/MolGenerator.html">
   17. Generative RNN in Browser
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  E. Contributed Chapters
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Hyperparameter_tuning.html">
   18. Hyperparameter Tuning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../applied/e3nn_traj.html">
   19. Equivariant Neural Network for Predicting Trajectories
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  F. Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../style.html">
   20. Style Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../changelog.html">
   21. Changelog
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  G. In Progress
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="molnets.html">
   22. Modern Molecular NNs
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  <script async defer src="https://api.dmol.pub/latest.js"></script><noscript><img src="https://api.dmol.pub/noscript.gif" alt="" referrerpolicy="no-referrer-when-downgrade" /></noscript> By <a href="https://twitter.com/andrewwhite01">Andrew White</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/dl/gnn.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/whitead/dmol-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/whitead/dmol-book/issues/new?title=Issue%20on%20page%20%2Fdl/gnn.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/whitead/dmol-book/blob/master/dl/gnn.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#representing-a-graph">
   8.1. Representing a Graph
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#running-this-notebook">
   8.2. Running This Notebook
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-graph-neural-network">
   8.3. A Graph Neural Network
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-simple-gnn">
     8.3.1. A simple GNN
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kipf-welling-gcn">
   8.4. Kipf &amp; Welling GCN
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gcn-implementation">
     8.4.1. GCN Implementation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#solubility-example">
   8.5. Solubility Example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#message-passing-viewpoint">
   8.6. Message Passing Viewpoint
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gated-graph-neural-network">
   8.7. Gated Graph Neural Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pooling">
   8.8. Pooling
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#readout-function">
   8.9. Readout Function
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intensive-vs-extensive">
     8.9.1. Intensive vs Extensive
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#battaglia-general-equations">
   8.10. Battaglia General Equations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reformulating-gcn-into-battaglia-equations">
     8.10.1. Reformulating GCN into Battaglia equations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-schnet-architecture">
   8.11. The SchNet Architecture
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#schnet-example-predicting-space-groups">
   8.12. SchNet Example: Predicting Space Groups
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#building-the-graphs">
     8.12.1. Building the graphs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementing-the-mlps">
     8.12.2. Implementing the MLPs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementing-the-gnn">
     8.12.3. Implementing the GNN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training">
     8.12.4. Training
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#current-research-directions">
   8.13. Current Research Directions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#common-architecture-motifs-and-comparisons">
     8.13.1. Common Architecture Motifs and Comparisons
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nodes-edges-and-features">
     8.13.2. Nodes, Edges, and Features
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#beyond-message-passing">
     8.13.3. Beyond Message Passing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#do-we-need-graphs">
     8.13.4. Do we need graphs?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stereochemistry-chiral-molecules">
     8.13.5. Stereochemistry/Chiral Molecules
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relevant-videos">
   8.14. Relevant Videos
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intro-to-gnns">
     8.14.1. Intro to GNNs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overview-of-gnn-with-molecule-compiler-examples">
     8.14.2. Overview of GNN with Molecule, Compiler Examples
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chapter-summary">
   8.15. Chapter Summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cited-references">
   8.16. Cited References
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Graph Neural Networks</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#representing-a-graph">
   8.1. Representing a Graph
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#running-this-notebook">
   8.2. Running This Notebook
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-graph-neural-network">
   8.3. A Graph Neural Network
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-simple-gnn">
     8.3.1. A simple GNN
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kipf-welling-gcn">
   8.4. Kipf &amp; Welling GCN
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gcn-implementation">
     8.4.1. GCN Implementation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#solubility-example">
   8.5. Solubility Example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#message-passing-viewpoint">
   8.6. Message Passing Viewpoint
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gated-graph-neural-network">
   8.7. Gated Graph Neural Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pooling">
   8.8. Pooling
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#readout-function">
   8.9. Readout Function
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intensive-vs-extensive">
     8.9.1. Intensive vs Extensive
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#battaglia-general-equations">
   8.10. Battaglia General Equations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reformulating-gcn-into-battaglia-equations">
     8.10.1. Reformulating GCN into Battaglia equations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-schnet-architecture">
   8.11. The SchNet Architecture
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#schnet-example-predicting-space-groups">
   8.12. SchNet Example: Predicting Space Groups
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#building-the-graphs">
     8.12.1. Building the graphs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementing-the-mlps">
     8.12.2. Implementing the MLPs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementing-the-gnn">
     8.12.3. Implementing the GNN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training">
     8.12.4. Training
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#current-research-directions">
   8.13. Current Research Directions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#common-architecture-motifs-and-comparisons">
     8.13.1. Common Architecture Motifs and Comparisons
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nodes-edges-and-features">
     8.13.2. Nodes, Edges, and Features
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#beyond-message-passing">
     8.13.3. Beyond Message Passing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#do-we-need-graphs">
     8.13.4. Do we need graphs?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stereochemistry-chiral-molecules">
     8.13.5. Stereochemistry/Chiral Molecules
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relevant-videos">
   8.14. Relevant Videos
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intro-to-gnns">
     8.14.1. Intro to GNNs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overview-of-gnn-with-molecule-compiler-examples">
     8.14.2. Overview of GNN with Molecule, Compiler Examples
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chapter-summary">
   8.15. Chapter Summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cited-references">
   8.16. Cited References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="graph-neural-networks">
<h1><span class="section-number">8. </span>Graph Neural Networks<a class="headerlink" href="#graph-neural-networks" title="Permalink to this headline">¶</a></h1>
<p>Historically, the biggest difficulty for machine learning with molecules was the choice and computation of “descriptors”. Graph neural networks (GNNs) are a category of deep neural networks whose inputs are graphs and provide a way around the choice of descriptors. A GNN can take a molecule directly as input.</p>
<div class="admonition-audience-objectives admonition">
<p class="admonition-title">Audience &amp; Objectives</p>
<p>This chapter builds on <a class="reference internal" href="layers.html"><span class="doc">Standard Layers</span></a> and <a class="reference internal" href="../ml/regression.html"><span class="doc">Regression &amp; Model Assessment</span></a>. Although it is defined here, it would be good to be familiarize yourself with graphs/networks. After completing this chapter, you should be able to</p>
<ul class="simple">
<li><p>Represent a molecule in a graph</p></li>
<li><p>Discuss and categorize common graph neural network architectures</p></li>
<li><p>Build a GNN and choose a read-out function for the type of labels</p></li>
<li><p>Distinguish between graph, edge, and node features</p></li>
<li><p>Formulate a GNN into edge-updates, node-updates, and aggregation steps</p></li>
</ul>
</div>
<p>GNNs are specific layers that input a graph and output a graph. You can find reviews of GNNs in Dwivedi <em>et al.</em><span id="id1">[<a class="reference internal" href="#id94" title="Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020.">DJL+20</a>]</span>, Bronstein <em>et al.</em><span id="id2">[<a class="reference internal" href="#id95" title="Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18–42, 2017.">BBL+17</a>]</span>, and  Wu <em>et al.</em><span id="id3">[<a class="reference internal" href="#id96" title="Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 2020.">WPC+20</a>]</span>. GNNs can be used for everything from coarse-grained molecular dynamics <span id="id4">[<a class="reference internal" href="#id108" title="Zhiheng Li, Geemi P Wellawatte, Maghesree Chakraborty, Heta A Gandhi, Chenliang Xu, and Andrew D White. Graph neural network based coarse-grained mapping prediction. Chemical Science, 11(35):9524–9531, 2020.">LWC+20</a>]</span> to predicting NMR chemical shifts <span id="id5">[<a class="reference internal" href="#id109" title="Ziyue Yang, Maghesree Chakraborty, and Andrew D White. Predicting chemical shifts with graph neural networks. bioRxiv, 2020.">YCW20</a>]</span> to modeling dynamics of solids <span id="id6">[<a class="reference internal" href="#id56" title="Tian Xie, Arthur France-Lanord, Yanming Wang, Yang Shao-Horn, and Jeffrey C Grossman. Graph dynamical networks for unsupervised learning of atomic scale dynamics in materials. Nature communications, 10(1):1–9, 2019.">XFLW+19</a>]</span>. Before we dive too deep into them, we must first understand how a graph is represented in a computer and how molecules are converted into graphs.</p>
<p>You can find an interactive introductory article on graphs and graph neural networks at <a class="reference external" href="https://distill.pub/2021/gnn-intro/">distill.pub</a> <span id="id7">[<a class="reference internal" href="#id192" title="Benjamin Sanchez-Lengeling, Emily Reif, Adam Pearce, and Alex Wiltschko. A gentle introduction to graph neural networks. Distill, 2021. https://distill.pub/2021/gnn-intro. doi:10.23915/distill.00033.">SLRPW21</a>]</span>. Most current research in GNNs is done with specialized deep learning libraries for graphs. As of 2022, the most common are <a class="reference external" href="https://pytorch-geometric.readthedocs.io/en/latest/">PyTorch Geometric</a>, <a class="reference external" href="https://www.dgl.ai/">Deep Graph library</a>, <a class="reference external" href="https://github.com/divelab/DIG">DIG</a>, <a class="reference external" href="https://graphneural.network/">Spektral</a>, and <a class="reference external" href="https://github.com/tensorflow/gnn">TensorFlow GNNS</a>.</p>
<section id="representing-a-graph">
<h2><span class="section-number">8.1. </span>Representing a Graph<a class="headerlink" href="#representing-a-graph" title="Permalink to this headline">¶</a></h2>
<p>A graph <span class="math notranslate nohighlight">\(\mathbf{G}\)</span> is a set of nodes <span class="math notranslate nohighlight">\(\mathbf{V}\)</span> and edges <span class="math notranslate nohighlight">\(\mathbf{E}\)</span>. In our setting, node <span class="math notranslate nohighlight">\(i\)</span> is defined by a vector <span class="math notranslate nohighlight">\(\vec{v}_i\)</span>, so that the set of nodes can be written as a rank 2 tensor. The edges can be represented as an adjacency matrix <span class="math notranslate nohighlight">\(\mathbf{E}\)</span>, where if <span class="math notranslate nohighlight">\(e_{ij} = 1\)</span> then nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are connected by an edge. In many fields, graphs are often immediately simplified to be directed and acyclic, which simplifies things. Molecules are instead undirected and have cycles (rings). Thus, our adjacency matrices are always symmetric <span class="math notranslate nohighlight">\(e_{ij} = e_{ji}\)</span> because there is no concept of direction in chemical bonds. Often our edges themselves have features, so that <span class="math notranslate nohighlight">\(e_{ij}\)</span> is itself a vector. Then the adjacency matrix becomes a rank 3 tensor. Examples of edge features might be covalent bond order or distance between two nodes.</p>
<figure class="align-default" id="methanol">
<a class="reference internal image-reference" href="../_images/methanol.jpg"><img alt="../_images/methanol.jpg" src="../_images/methanol.jpg" style="width: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.1 </span><span class="caption-text">Methanol with atoms numbered so that we can convert it to a graph.</span><a class="headerlink" href="#methanol" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<aside class="margin sidebar">
<p class="sidebar-title">one-hot</p>
<p>Recall that a one-hot is a vector of
all 0s and a single 1 - <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">0,</span> <span class="pre">0]</span></code>. The
index of the non-zero element indictates the class.
In this case, class is element.</p>
</aside>
<p>Let’s see how a graph can be constructed from a molecule. Consider methanol, shown in <a class="reference internal" href="#methanol"><span class="std std-numref">Fig. 8.1</span></a>. I’ve numbered the atoms so that we have an order for defining the nodes/edges. First, the node features. You can use anything for node features, but often we’ll begin with one-hot encoded feature vectors:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Node</p></th>
<th class="head"><p>C</p></th>
<th class="head"><p>H</p></th>
<th class="text-align:right head"><p>O</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>2</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>3</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>4</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>5</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>6</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
</tbody>
</table>
<p><span class="math notranslate nohighlight">\(\mathbf{V}\)</span> will be the combined feature vectors of these nodes. The adjacency matrix <span class="math notranslate nohighlight">\(\mathbf{E}\)</span> will look like:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="head"><p>1</p></th>
<th class="head"><p>2</p></th>
<th class="head"><p>3</p></th>
<th class="head"><p>4</p></th>
<th class="head"><p>5</p></th>
<th class="text-align:right head"><p>6</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>2</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>3</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>4</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>5</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>6</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
</tbody>
</table>
<p>Take a moment to understand these two. For example, notice that rows 1, 2, and 3 only have the 4th column as non-zero. That’s because atoms 1-3 are bonded only to carbon (atom 4). Also, the diagonal is always 0 because atoms cannot be bonded with themselves.</p>
<p>You can find a similar process for converting crystals into graphs in Xie et al. <span id="id8">[<a class="reference internal" href="#id132" title="Tian Xie and Jeffrey C. Grossman. Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. Phys. Rev. Lett., 120:145301, Apr 2018. URL: https://link.aps.org/doi/10.1103/PhysRevLett.120.145301, doi:10.1103/PhysRevLett.120.145301.">XG18</a>]</span>. We’ll now begin with a function which can convert a smiles string into this representation.</p>
</section>
<section id="running-this-notebook">
<h2><span class="section-number">8.2. </span>Running This Notebook<a class="headerlink" href="#running-this-notebook" title="Permalink to this headline">¶</a></h2>
<p>Click the  <i aria-label="Launch interactive content" class="fas fa-rocket"></i>  above to launch this page as an interactive Google Colab. See details below on installing packages.</p>
<div class="dropdown admonition tip">
<p class="admonition-title">Tip</p>
<p>To install packages, execute this code in a new cell.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install dmol-book
</pre></div>
</div>
<p>If you find install problems, you can get the latest working versions of packages used in <a class="reference external" href="https://github.com/whitead/dmol-book/blob/master/package/setup.py">this book here</a></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">rdkit</span><span class="o">,</span> <span class="nn">rdkit.Chem</span><span class="o">,</span> <span class="nn">rdkit.Chem.rdDepictor</span><span class="o">,</span> <span class="nn">rdkit.Chem.Draw</span>
<span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>
<span class="kn">import</span> <span class="nn">dmol</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">soldata</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="s2">&quot;https://github.com/whitead/dmol-book/raw/master/data/curated-solubility-dataset.csv&quot;</span>
<span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">my_elements</span> <span class="o">=</span> <span class="p">{</span><span class="mi">6</span><span class="p">:</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="mi">8</span><span class="p">:</span> <span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;H&quot;</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>The hidden cell below defines our function <code class="docutils literal notranslate"><span class="pre">smiles2graph</span></code>. This creates one-hot node feature vectors for the element C, H, and O. It also creates an adjacency tensor with one-hot bond order being the feature vector.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">smiles2graph</span><span class="p">(</span><span class="n">sml</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Argument for the RD2NX function should be a valid SMILES sequence</span>
<span class="sd">    returns: the graph</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">MolFromSmiles</span><span class="p">(</span><span class="n">sml</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">AddHs</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">order_string</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">SINGLE</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">DOUBLE</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">TRIPLE</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">AROMATIC</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">GetAtoms</span><span class="p">()))</span>
    <span class="n">nodes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">my_elements</span><span class="p">)))</span>
    <span class="n">lookup</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">my_elements</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">GetAtoms</span><span class="p">():</span>
        <span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">GetIdx</span><span class="p">(),</span> <span class="n">lookup</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">GetAtomicNum</span><span class="p">())]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">adj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">GetBonds</span><span class="p">():</span>
        <span class="n">u</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">j</span><span class="o">.</span><span class="n">GetBeginAtomIdx</span><span class="p">(),</span> <span class="n">j</span><span class="o">.</span><span class="n">GetEndAtomIdx</span><span class="p">())</span>
        <span class="n">v</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">j</span><span class="o">.</span><span class="n">GetBeginAtomIdx</span><span class="p">(),</span> <span class="n">j</span><span class="o">.</span><span class="n">GetEndAtomIdx</span><span class="p">())</span>
        <span class="n">order</span> <span class="o">=</span> <span class="n">j</span><span class="o">.</span><span class="n">GetBondType</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">order</span> <span class="ow">in</span> <span class="n">order_string</span><span class="p">:</span>
            <span class="n">order</span> <span class="o">=</span> <span class="n">order_string</span><span class="p">[</span><span class="n">order</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Warning</span><span class="p">(</span><span class="s2">&quot;Ignoring bond order&quot;</span> <span class="o">+</span> <span class="n">order</span><span class="p">)</span>
        <span class="n">adj</span><span class="p">[</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">order</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">adj</span><span class="p">[</span><span class="n">v</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">order</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span> <span class="o">=</span> <span class="n">smiles2graph</span><span class="p">(</span><span class="s2">&quot;CO&quot;</span><span class="p">)</span>
<span class="n">nodes</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.],
       [0., 0., 1.],
       [0., 0., 1.],
       [0., 0., 1.]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="a-graph-neural-network">
<h2><span class="section-number">8.3. </span>A Graph Neural Network<a class="headerlink" href="#a-graph-neural-network" title="Permalink to this headline">¶</a></h2>
<p>A graph neural network (GNN) is a neural network with two defining attributes:</p>
<ol class="simple">
<li><p>Its input is a graph</p></li>
<li><p>Its output is permutation equivariant</p></li>
</ol>
<p>We can understand clearly the first point. Here, a graph permutation means re-ordering our nodes. In our methanol example above, we could have easily made the carbon be atom 1 instead of atom 4. Our new adjacency matrix would then be:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="head"><p>1</p></th>
<th class="head"><p>2</p></th>
<th class="head"><p>3</p></th>
<th class="head"><p>4</p></th>
<th class="head"><p>5</p></th>
<th class="text-align:right head"><p>6</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>2</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>3</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>4</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>5</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>6</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
</tbody>
</table>
<p>A GNN is permutation equivariant if the output change the same way as these exchanges. If you are trying to model a per-atom quantity like partial charge or chemical shift, this is obviously essential. If you change the order of atoms input, you would expect the order of their partial charges to similarly change.</p>
<p>Often we want to model a whole-molecule property, like solubility or energy. This should be <strong>invariant</strong> to changing the order of the atoms. To make an equivariant model invariant, we use read-outs (defined below). See <a class="reference internal" href="data.html"><span class="doc">Input Data &amp; Equivariances</span></a> for a  more detailed discussion of equivariance.</p>
<section id="a-simple-gnn">
<h3><span class="section-number">8.3.1. </span>A simple GNN<a class="headerlink" href="#a-simple-gnn" title="Permalink to this headline">¶</a></h3>
<p>We will often mention a GNN when we really mean a layer from a GNN. Most GNNs implement a specific layer that can deal with graphs, and so usually we are only concerned with this layer. Let’s see an example of a simple layer for a GNN:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e02b5ca5-bf0a-419f-b903-99d609686040">
<span class="eqno">(8.1)<a class="headerlink" href="#equation-e02b5ca5-bf0a-419f-b903-99d609686040" title="Permalink to this equation">¶</a></span>\[\begin{equation}
f_k = \sigma\left( \sum_i \sum_j v_{ij}w_{jk}  \right)
\end{equation}\]</div>
<p>This equation shows that we first multiply every node (<span class="math notranslate nohighlight">\(v_{ij}\)</span>) feature by trainable weights <span class="math notranslate nohighlight">\(w_{jk}\)</span>, sum over all node features, and then apply an activation. This will yield a single feature vector for the graph. Is this equation permutation equivariant? Yes, because the node index in our expression is index <span class="math notranslate nohighlight">\(i\)</span> which can be re-ordered without affecting the output.</p>
<p>Let’s see an example that is similar, but not permutation equivariant:</p>
<div class="amsmath math notranslate nohighlight" id="equation-204ed4d8-4ba5-4692-9360-e144d8df8f80">
<span class="eqno">(8.2)<a class="headerlink" href="#equation-204ed4d8-4ba5-4692-9360-e144d8df8f80" title="Permalink to this equation">¶</a></span>\[\begin{equation}
f_k = \sigma\left( \sum_i v_{ij}w_{ik}  \right)
\end{equation}\]</div>
<p>This is a small change. We have one weight vector per node now. This makes the trainable weights depend on the ordering of the nodes. Then if we swap the node ordering, our weights will no longer align. So if we were to input two methanol molecules, which should have the same output, but we switched two atom numbers, we would get different answers. These simple examples differ from real GNNs in two important ways: (i) they give a single feature vector output, which throws away per-node information, and (ii) they do not use the adjacency matrix. Let’s see a real GNN that has these properties while maintaining permutation equivariant.</p>
</section>
</section>
<section id="kipf-welling-gcn">
<h2><span class="section-number">8.4. </span>Kipf &amp; Welling GCN<a class="headerlink" href="#kipf-welling-gcn" title="Permalink to this headline">¶</a></h2>
<p>One of the first popular GNNs was the Kipf &amp; Welling graph convolutional network (GCN) <span id="id9">[<a class="reference internal" href="#id73" title="Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.">KW16</a>]</span>. Although some people consider GCNs to be a broad class of GNNs, we’ll use GCNs to refer specifically the Kipf &amp; Welling GCN.
Thomas Kipf has written an <a class="reference external" href="https://tkipf.github.io/graph-convolutional-networks/">excellent article introducing the GCN</a>.</p>
<p>The input to a GCN layer is <span class="math notranslate nohighlight">\(\mathbf{V}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{E}\)</span> and it outputs an updated <span class="math notranslate nohighlight">\(\mathbf{V}'\)</span>. Each node feature vector is updated. The way it updates a node feature vector is by averaging the feature vectors of its neighbors, as determined by <span class="math notranslate nohighlight">\(\mathbf{E}\)</span>. The choice of averaging over neighbors is what makes a GCN layer permutation equivariant. Averaging over neighbors is not trainable, so we must add trainable parameters. We multiply the neighbor features by a trainable matrix before the averaging, which gives the GCN the ability to learn. In Einstein notation, this process is:</p>
<div class="math notranslate nohighlight" id="equation-gcn">
<span class="eqno">(8.3)<a class="headerlink" href="#equation-gcn" title="Permalink to this equation">¶</a></span>\[
v_{il} = \sigma\left(\frac{1}{d_i}e_{ij}v_{jk}w_{lk}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(i\)</span> is the node we’re considering, <span class="math notranslate nohighlight">\(j\)</span> is the neighbor index, <span class="math notranslate nohighlight">\(k\)</span> is the node input feature, <span class="math notranslate nohighlight">\(l\)</span> is the output node feature, <span class="math notranslate nohighlight">\(d_i\)</span> is the degree of node i (which makes it an average instead of sum), <span class="math notranslate nohighlight">\(e_{ij}\)</span> isolates neighbors so that all non-neighbor <span class="math notranslate nohighlight">\(v_{jk}\)</span>s are zero, <span class="math notranslate nohighlight">\(\sigma\)</span> is our activation, and <span class="math notranslate nohighlight">\(w_{lk}\)</span> is the trainable weights. This equation is a mouthful, but it truly just is the average over neighbors with a trainable matrix thrown in. One common modification is to make all nodes neighbors of themselves. This is so that the output node features <span class="math notranslate nohighlight">\(v_{il}\)</span> depends on the input features <span class="math notranslate nohighlight">\(v_{ik}\)</span>. We do not need to change our equation, just make the adjacency matrix have <span class="math notranslate nohighlight">\(1\)</span>s on the diagonal instead of <span class="math notranslate nohighlight">\(0\)</span> by adding the identity matrix during pre-processing.</p>
<p>Building understanding about the GCN is important for understanding other GNNs. You can view the GCN layer as a way to “communicate” between a node and its neighbors. The output for node <span class="math notranslate nohighlight">\(i\)</span> will depend only on its immediate neighbors. For chemistry, this is not satisfactory. You can stack multiple layers though. If you have two layers, the output for node <span class="math notranslate nohighlight">\(i\)</span> will include information about node <span class="math notranslate nohighlight">\(i\)</span>’s neighbors’ neighbors. Another important detail to understand in GCNs is that the averaging procedure accomplishes two goals: (i) it gives permutation equivariance by removing the effect of neighbor order and (ii) it prevents a change in magnitude in node features. A sum would accomplish (i) but would cause the magnitude of the node features to grow after each layer. Of course, you could ad-hoc put a batch normalization layer after each GCN layer to keep output magnitudes stable but averaging is easy.</p>
<figure class="align-default" id="dframe">
<div class="cell_output docutils container">
<img alt="../_images/gnn_11_0.png" src="../_images/gnn_11_0.png" />
</div>
<figcaption>
<p><span class="caption-number">Fig. 8.2 </span><span class="caption-text">Intermediate step of the graph convolution layer. The 3D vectors are the node features and start as one-hot, so a <code class="docutils literal notranslate"><span class="pre">[1.00,</span> <span class="pre">0.00,</span> <span class="pre">0.00]</span></code> means hydrogen. The center node will be updated by averaging its neighbors features.</span><a class="headerlink" href="#dframe" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>To help understand the GCN layer, look at <a class="reference internal" href="#dframe"><span class="std std-numref">Fig. 8.2</span></a>. It shows an intermediate step of the GCN layer. Each node feature is represented here as a one-hot encoded vector at input. The animation in <a class="reference internal" href="#gcnanim"><span class="std std-numref">Fig. 8.3</span></a> shows the averaging process over neighbor features.  To make this animation easy to follow, the trainable weights and activation functions are not considered. Note that the animation repeats for a second layer. Watch how the “information” about there being an oxygen atom in the molecule is propagated only after two layers to each atom. All GNNs operate with similar approaches, so try to understand how this animation works.</p>
<figure class="align-default" id="gcnanim">
<img alt="../_images/gcn.gif" src="../_images/gcn.gif" />
<figcaption>
<p><span class="caption-number">Fig. 8.3 </span><span class="caption-text">Animation of the graph convolution layer operation. The left is input, right is output node features. Note that two layers are shown (see title change). As the animation plays out, you can see how the information about the atoms propagates through the molecule via the averaging over neigbhors. So the oxygen goes from being just an oxygen, to an oxygen bonded to C and H, to an oxygen bonded to an H and CH3. The colors just reflect the same information in the numerical values.</span><a class="headerlink" href="#gcnanim" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<section id="gcn-implementation">
<h3><span class="section-number">8.4.1. </span>GCN Implementation<a class="headerlink" href="#gcn-implementation" title="Permalink to this headline">¶</a></h3>
<p>Let’s now create a tensor implementation of the GCN. We’ll skip the activation and trainable weights for now.
We must first compute our rank 2 adjacency matrix. The <code class="docutils literal notranslate"><span class="pre">smiles2graph</span></code> code above computes an adjacency tensor with feature vectors. We can fix that with a simple reduction and add the identity at the same time</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span> <span class="o">=</span> <span class="n">smiles2graph</span><span class="p">(</span><span class="s2">&quot;CO&quot;</span><span class="p">)</span>
<span class="n">adj_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">adj</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">adj</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">adj_mat</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1., 1., 1., 1., 1., 0.],
       [1., 1., 0., 0., 0., 1.],
       [1., 0., 1., 0., 0., 0.],
       [1., 0., 0., 1., 0., 0.],
       [1., 0., 0., 0., 1., 0.],
       [0., 1., 0., 0., 0., 1.]])
</pre></div>
</div>
</div>
</div>
<p>To compute degree of each node, we can do another reduction:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">degree</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">adj_mat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">degree</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([5., 3., 2., 2., 2., 2.])
</pre></div>
</div>
</div>
</div>
<p>Now we can put all these pieces together into the Einstein equation</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1"># note to divide by degree, make the input 1 / degree</span>
<span class="n">new_nodes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;i,ij,jk-&gt;ik&quot;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">degree</span><span class="p">,</span> <span class="n">adj_mat</span><span class="p">,</span> <span class="n">nodes</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">new_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1. 0. 0.]
[0.2 0.2 0.6]
</pre></div>
</div>
</div>
</div>
<p>To now implement this as a layer in Keras, we must put this code above into a new Layer subclass. The code is relatively straightforward, but you can read-up on the function names and Layer class in <a class="reference external" href="https://keras.io/guides/making_new_layers_and_models_via_subclassing/">this tutorial</a>. The three main changes are that we create trainable parameters <code class="docutils literal notranslate"><span class="pre">self.w</span></code> and use them in the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/einsum" title="(in TensorFlow v2.8)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.einsum</span></code></a>, we use an activation <code class="docutils literal notranslate"><span class="pre">self.activation</span></code>, and we output both our new node features and the adjacency matrix. The reason to output the adjacency matrix is so that we can stack multiple GCN layers without having to pass the adjacency matrix each time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GCNLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Implementation of GCN as layer&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># constructor, which just calls super constructor</span>
        <span class="c1"># and turns requested activation into a callable function</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GCNLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="c1"># create trainable weights</span>
        <span class="n">node_shape</span><span class="p">,</span> <span class="n">adj_shape</span> <span class="o">=</span> <span class="n">input_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">node_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">node_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;w&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># split input into nodes, adj</span>
        <span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="c1"># compute degree</span>
        <span class="n">degree</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">adj</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># GCN equation</span>
        <span class="n">new_nodes</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bi,bij,bjk,kl-&gt;bil&quot;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">degree</span><span class="p">,</span> <span class="n">adj</span><span class="p">,</span> <span class="n">nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">new_nodes</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">adj</span>
</pre></div>
</div>
</div>
</div>
<p>A lot of the code above is Keras/TF specific and getting the variables to the right place. There are really only two key lines here. The first is to compute the degree by summing over the columns of the adjacency matrix:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">degree</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">adj</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>The second key line is to do the GCN equation <a class="reference internal" href="#equation-gcn">(8.3)</a> (without the activation)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">new_nodes</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bi,bij,bjk,kl-&gt;bil&quot;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">degree</span><span class="p">,</span> <span class="n">adj</span><span class="p">,</span> <span class="n">nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
<p>We can now try our layer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gcnlayer</span> <span class="o">=</span> <span class="n">GCNLayer</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
<span class="c1"># we insert a batch axis here</span>
<span class="n">gcnlayer</span><span class="p">((</span><span class="n">nodes</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">adj_mat</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;tf.Tensor: shape=(1, 6, 3), dtype=float32, numpy=
 array([[[0.        , 0.46567526, 0.07535715],
         [0.        , 0.12714943, 0.05325063],
         [0.01475453, 0.295794  , 0.39316285],
         [0.01475453, 0.295794  , 0.39316285],
         [0.01475453, 0.295794  , 0.39316285],
         [0.        , 0.38166213, 0.        ]]], dtype=float32)&gt;,
 &lt;tf.Tensor: shape=(1, 6, 6), dtype=float32, numpy=
 array([[[1., 1., 1., 1., 1., 0.],
         [1., 1., 0., 0., 0., 1.],
         [1., 0., 1., 0., 0., 0.],
         [1., 0., 0., 1., 0., 0.],
         [1., 0., 0., 0., 1., 0.],
         [0., 1., 0., 0., 0., 1.]]], dtype=float32)&gt;)
</pre></div>
</div>
</div>
</div>
<p>It outputs (1) the new node features and (2) the adjacency matrix. Let’s make sure we can stack these and apply the GCN multiple times</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">adj_mat</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">gcnlayer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;tf.Tensor: shape=(1, 6, 3), dtype=float32, numpy=
array([[[0.        , 0.18908624, 0.        ],
        [0.        , 0.        , 0.        ],
        [0.        , 0.145219  , 0.        ],
        [0.        , 0.145219  , 0.        ],
        [0.        , 0.145219  , 0.        ],
        [0.        , 0.        , 0.        ]]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(1, 6, 6), dtype=float32, numpy=
array([[[1., 1., 1., 1., 1., 0.],
        [1., 1., 0., 0., 0., 1.],
        [1., 0., 1., 0., 0., 0.],
        [1., 0., 0., 1., 0., 0.],
        [1., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 1.]]], dtype=float32)&gt;)
</pre></div>
</div>
</div>
</div>
<p>It works! Why do we see zeros though? Probably because we had negative numbers that were removed by our ReLU activation. This will be solved by training and increasing our dimension number.</p>
</section>
</section>
<section id="solubility-example">
<h2><span class="section-number">8.5. </span>Solubility Example<a class="headerlink" href="#solubility-example" title="Permalink to this headline">¶</a></h2>
<p>We’ll now revisit predicting solubility with GCNs. Remember before that we used the features included with the dataset. Now we can use the molecular structures directly. Our GCN layer outputs node-level features. To predict solubility, we need to get a graph-level feature. We’ll see later how to be more sophisticated in this process, but for now let’s just take the average over all node features after our GCN layers. This is simple, permutation invariant, and gets us from node-level to graph level. Here’s an implementation of this</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GRLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A GNN layer that computes average over all node features&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;GRLayer&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GRLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">reduction</span>
</pre></div>
</div>
</div>
</div>
<p>The key line in that code is to just to compute the mean over the nodes (<code class="docutils literal notranslate"><span class="pre">axis=1</span></code>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">reduction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>To complete our deep solubility predictor, we can add some dense layers and make sure we have a single-output without activation since we’re doing regression. Note this model is defined using the <a class="reference external" href="https://keras.io/guides/functional_api/">Keras functional API</a> which is necessary when you have multiple inputs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ninput</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span>
    <span class="p">(</span>
        <span class="kc">None</span><span class="p">,</span>
        <span class="mi">100</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">ainput</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span>
    <span class="p">(</span>
        <span class="kc">None</span><span class="p">,</span>
        <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="c1"># GCN block</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">GCNLayer</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">)([</span><span class="n">ninput</span><span class="p">,</span> <span class="n">ainput</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">GCNLayer</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">GCNLayer</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">GCNLayer</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># reduce to graph features</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">GRLayer</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># standard layers (the readout)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="s2">&quot;tanh&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="n">ninput</span><span class="p">,</span> <span class="n">ainput</span><span class="p">),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>where does the 100 come from? Well, this dataset has lots of elements so we cannot use our size 3 one-hot encodings because we’ll have more than 3 unique elements. We previously only had C, H and O. This is a good time to update our <code class="docutils literal notranslate"><span class="pre">smiles2graph</span></code> function to deal with this.</p>
<div class="cell tag_hidden-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gen_smiles2graph</span><span class="p">(</span><span class="n">sml</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Argument for the RD2NX function should be a valid SMILES sequence</span>
<span class="sd">    returns: the graph</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">MolFromSmiles</span><span class="p">(</span><span class="n">sml</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">AddHs</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">order_string</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">SINGLE</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">DOUBLE</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">TRIPLE</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">AROMATIC</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">GetAtoms</span><span class="p">()))</span>
    <span class="n">nodes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">GetAtoms</span><span class="p">():</span>
        <span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">GetIdx</span><span class="p">(),</span> <span class="n">i</span><span class="o">.</span><span class="n">GetAtomicNum</span><span class="p">()]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">adj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">GetBonds</span><span class="p">():</span>
        <span class="n">u</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">j</span><span class="o">.</span><span class="n">GetBeginAtomIdx</span><span class="p">(),</span> <span class="n">j</span><span class="o">.</span><span class="n">GetEndAtomIdx</span><span class="p">())</span>
        <span class="n">v</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">j</span><span class="o">.</span><span class="n">GetBeginAtomIdx</span><span class="p">(),</span> <span class="n">j</span><span class="o">.</span><span class="n">GetEndAtomIdx</span><span class="p">())</span>
        <span class="n">order</span> <span class="o">=</span> <span class="n">j</span><span class="o">.</span><span class="n">GetBondType</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">order</span> <span class="ow">in</span> <span class="n">order_string</span><span class="p">:</span>
            <span class="n">order</span> <span class="o">=</span> <span class="n">order_string</span><span class="p">[</span><span class="n">order</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Warning</span><span class="p">(</span><span class="s2">&quot;Ignoring bond order&quot;</span> <span class="o">+</span> <span class="n">order</span><span class="p">)</span>
        <span class="n">adj</span><span class="p">[</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">adj</span><span class="p">[</span><span class="n">v</span><span class="p">,</span> <span class="n">u</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">adj</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span> <span class="o">=</span> <span class="n">gen_smiles2graph</span><span class="p">(</span><span class="s2">&quot;CO&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="p">((</span><span class="n">nodes</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">adj_mat</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.0107595]], dtype=float32)&gt;
</pre></div>
</div>
</div>
</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>We have switched from adjacency tensor to matrix only because a GCN cannot use edge features. Other architectures though can.</p>
</aside>
<p>It outputs one number! That’s always nice to have. Now we need to do some work to get a trainable dataset. Our dataset is a little bit complex because our features are tuples of tensors(<span class="math notranslate nohighlight">\(\mathbf{V}, \mathbf{E}\)</span>) so that our dataset is a tuple of tuples: <span class="math notranslate nohighlight">\(\left((\mathbf{V}, \mathbf{E}), y\right)\)</span>. We use a <strong>generator</strong>, which is just a python function that can return multiple times. Our function returns once for every training example. Then we have to pass it to the <code class="docutils literal notranslate"><span class="pre">from_generator</span></code> <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" title="(in TensorFlow v2.8)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code></a> constructor which requires explicit declaration of the shapes of these examples.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">example</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">soldata</span><span class="p">)):</span>
        <span class="n">graph</span> <span class="o">=</span> <span class="n">gen_smiles2graph</span><span class="p">(</span><span class="n">soldata</span><span class="o">.</span><span class="n">SMILES</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">sol</span> <span class="o">=</span> <span class="n">soldata</span><span class="o">.</span><span class="n">Solubility</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">yield</span> <span class="n">graph</span><span class="p">,</span> <span class="n">sol</span>


<span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_generator</span><span class="p">(</span>
    <span class="n">example</span><span class="p">,</span>
    <span class="n">output_types</span><span class="o">=</span><span class="p">((</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="n">output_shapes</span><span class="o">=</span><span class="p">(</span>
        <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">]),</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">])),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([]),</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Whew, that’s a lot. Now we can do our usual splitting of the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">val_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="mi">400</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And finally, time to train.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mean_squared_error&quot;</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">val_data</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gnn_39_0.png" src="../_images/gnn_39_0.png" />
</div>
</div>
<p>This model is definitely underfit. One reason is that our batch size is 1. This is a side-effect of making the number of atoms variable and then Keras/tensorflow has trouble batching together our data if there are two unknown dimensions. A standard trick is to group together multiple molecules into one graph, but making sure they are disconnected (no bonds between the molecules). That allows you to batch molecules without increasing the rank of your model/data.</p>
<p>Let’s now check the parity plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">test_y</span> <span class="o">=</span> <span class="p">[</span><span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_y</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
    <span class="nb">min</span><span class="p">(</span><span class="n">test_y</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
    <span class="nb">max</span><span class="p">(</span><span class="n">test_y</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;correlation = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">test_y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
    <span class="nb">min</span><span class="p">(</span><span class="n">test_y</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
    <span class="nb">max</span><span class="p">(</span><span class="n">test_y</span><span class="p">)</span> <span class="o">-</span> <span class="mi">3</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;loss = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">test_y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Testing Data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gnn_41_0.png" src="../_images/gnn_41_0.png" />
</div>
</div>
</section>
<section id="message-passing-viewpoint">
<h2><span class="section-number">8.6. </span>Message Passing Viewpoint<a class="headerlink" href="#message-passing-viewpoint" title="Permalink to this headline">¶</a></h2>
<p>One way to more broadly view a GCN layer is that it is a kind of “message-passing” layer. You first compute a message coming from each neighboring node:</p>
<div class="amsmath math notranslate nohighlight" id="equation-6ce547aa-e524-44fc-a9be-f1f5e5273286">
<span class="eqno">(8.4)<a class="headerlink" href="#equation-6ce547aa-e524-44fc-a9be-f1f5e5273286" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{e}_{{s_i}j} = \vec{v}_{{s_i}j} \mathbf{W}
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(v_{{s_i}j}\)</span> means the <span class="math notranslate nohighlight">\(j\)</span>th neighbor of node <span class="math notranslate nohighlight">\(i\)</span>. The <span class="math notranslate nohighlight">\(s_i\)</span> means senders to <span class="math notranslate nohighlight">\(i\)</span>. This is how a GCN computes the messages, it’s just a weight matrix times each neighbor node features. After getting the messages that will go to node <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(\vec{e}_{{s_i}j}\)</span>, we aggregate them using a function which is permutation invariant to the order of neighbors:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f5413625-b250-4deb-b890-2e788ccb8ed9">
<span class="eqno">(8.5)<a class="headerlink" href="#equation-f5413625-b250-4deb-b890-2e788ccb8ed9" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{e}_{i} = \frac{1}{|\vec{e}_{{s_i}j}|}\sum \vec{e}_{{s_i}j} 
\end{equation}\]</div>
<p>In the GCN this aggregation is just a mean, but it can be any permutation invariant (possibly trainable) function. Finally, we update our node using the aggregated message in the GCN:</p>
<div class="amsmath math notranslate nohighlight" id="equation-bada5871-92d8-491d-a5d9-ebfee41318b8">
<span class="eqno">(8.6)<a class="headerlink" href="#equation-bada5871-92d8-491d-a5d9-ebfee41318b8" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{v}^{'}_{i} = \sigma(\vec{e}_i)
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(v^{'}\)</span> indicates the new node features. This is simply the activated aggregated message. Writing it out this way, you can see how it is possible to make small changes. One important paper by Gilmer et al. explored some of these choices and described how this general idea of message passing layers does well in learning to predict molecular energies from quantum mechanics <span id="id10">[<a class="reference internal" href="#id77" title="Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.">GSR+17</a>]</span>. Examples of changes to the above GCN equations are to include edge information when computing the neighbor messages or use a dense neural network layer in place of <span class="math notranslate nohighlight">\(\sigma\)</span>. You can think of the GCN as one type of a broader class of message passing graph neural networks, sometimes abbreviated as MPNN.</p>
</section>
<section id="gated-graph-neural-network">
<h2><span class="section-number">8.7. </span>Gated Graph Neural Network<a class="headerlink" href="#gated-graph-neural-network" title="Permalink to this headline">¶</a></h2>
<p>One common variant of the message passing layer is the <strong>gated graph neural network</strong> (GGN) <span id="id11">[<a class="reference internal" href="#id75" title="Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.">LTBZ15</a>]</span>. It replaces the last equation, the node update, with</p>
<div class="amsmath math notranslate nohighlight" id="equation-91dccac1-0d5a-422e-81dc-c846a50b9624">
<span class="eqno">(8.7)<a class="headerlink" href="#equation-91dccac1-0d5a-422e-81dc-c846a50b9624" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{v}^{'}_{i} = \textrm{GRU}(\vec{v}_i, \vec{e}_i)
\end{equation}\]</div>
<p>where the <span class="math notranslate nohighlight">\(\textrm{GRU}(\cdot, \cdot)\)</span> is a gated recurrent unit<span id="id12">[<a class="reference internal" href="#id76" title="Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.">CGCB14</a>]</span>. A GRU is a binary (two input arguments) neural network that is typically used in sequence modeling. The interesting property of a GGN relative to a GCN is that it has trainable parameters in the node update (from the GRU), giving the model a bit more flexibility. In a GGN, the GRU parameters are kept the same at each layer, like how a GRU is used to model sequences. What’s nice about this is that you can stack infinite GGN layers without increasing the number of trainable parameters (assuming you make <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> the same at each layer). Thus GGNs are suited for large graphs, like a large protein or large unit cell.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>You’ll often see the prefix “gated” on GNNs and that means that the nodes are updated according to a GRU.</p>
</aside>
</section>
<section id="pooling">
<h2><span class="section-number">8.8. </span>Pooling<a class="headerlink" href="#pooling" title="Permalink to this headline">¶</a></h2>
<p>Within the message passing viewpoint, and in general for GNNS, the way that messages from neighbors are combined is a key step. This is sometimes called <strong>pooling</strong>, since it’s similar to the pooling layer used in convolutional neural networks. Just like in pooling for convolutional neural networks, there are multiple reduction operations you can use. Typically you see a sum or mean reduction in GNNs, but you can be quite sophisticated like in the Graph Isomorphism Networks <span id="id13">[<a class="reference internal" href="#id90" title="Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations. 2018.">XHLJ18</a>]</span>. We’ll see an example in our attention chapter of using self-attention, which can also be used for pooling. It can be tempting to focus on this step, but it’s been empirically found that the choice of pooling is not so important<span id="id14">[<a class="reference internal" href="#id89" title="Enxhell Luzhnica, Ben Day, and Pietro Liò. On graph classification networks, datasets and baselines. arXiv preprint arXiv:1905.04682, 2019.">LDLio19</a>, <a class="reference internal" href="#id88" title="Diego Mesquita, Amauri Souza, and Samuel Kaski. Rethinking pooling in graph neural networks. Advances in Neural Information Processing Systems, 2020.">MSK20</a>]</span>. The key property of the pooling is permutation <em>invariance</em> - we want the aggregation operation to not depend on order of nodes (or edges if pooling over them). You can find a recent review of pooling methods in Grattarola et al. <span id="id15">[<a class="reference internal" href="#id196" title="Daniele Grattarola, Daniele Zambon, Filippo Maria Bianchi, and Cesare Alippi. Understanding pooling in graph neural networks. arXiv preprint arXiv:2110.05292, 2021.">GZBA21</a>]</span>.</p>
<p>You can see a more visual comparison and overview of the various pooling strategies in this distill article by Daigavane et al. <span id="id16">[<a class="reference internal" href="#id198" title="Ameya Daigavane, Balaraman Ravindran, and Gaurav Aggarwal. Understanding convolutions on graphs. Distill, 2021. https://distill.pub/2021/understanding-gnns. doi:10.23915/distill.00032.">DRA21</a>]</span>.</p>
</section>
<section id="readout-function">
<h2><span class="section-number">8.9. </span>Readout Function<a class="headerlink" href="#readout-function" title="Permalink to this headline">¶</a></h2>
<p>GNNs output a graph by design. It is rare that our labels are graphs – typically we have node labels or a single graph label. An example of a node label is partial charge of atoms. An example of a graph label would be the energy of the molecule. The process of converting the graph output from the GNN into our predicted node labels or graph label is called the <strong>readout</strong>. If we have node labels, we can simply discard the edges and use our output node feature vectors from the GNN as the prediction, perhaps with a few dense layers before our predicted output label.</p>
<p>If we’re trying to predict a graph-level label like energy of the molecule or net charge, we need to be careful when converting from node/edge features to a graph label. If we simply put the node features into a dense layer to get to the desired shape graph label, we will lose permutation equivariance (technically it’s permutation invariance now since our output is graph label, not node labels). The readout we did above in the solubility example was a reduction over the node features to get a graph feature. Then we used this graph feature in dense layers. It turns out this is the only way <span id="id17">[<a class="reference internal" href="#id115" title="Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in neural information processing systems, 3391–3401. 2017.">ZKR+17</a>]</span> to do a graph feature readout: a reduction over nodes to get graph feature and then dense layers to get predicted graph label from those graph features. You can also do some dense layers on the node features individually, but that already happens in GNN so I do not recommend it. This readout is sometimes called DeepSets because it is the same form as the DeepSets architecture, which is a permutation invariant architecture for features that are sets<span id="id18">[<a class="reference internal" href="#id115" title="Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in neural information processing systems, 3391–3401. 2017.">ZKR+17</a>]</span>.</p>
<p>You may notice that the pooling and readouts both use permutation invariant functions. Thus, DeepSets can be used for pooling and attention could be used for readouts.</p>
<section id="intensive-vs-extensive">
<h3><span class="section-number">8.9.1. </span>Intensive vs Extensive<a class="headerlink" href="#intensive-vs-extensive" title="Permalink to this headline">¶</a></h3>
<p>One important consideration of a readout in regression is if your labels are <strong>intensive</strong> or <strong>extensive</strong>. An intensive label is one whose value is independent of the number of nodes (or atoms). For example, the index of refraction or solubility are intensive. The readout for an intensive label should (generally) be independent of the number of a nodes/atoms. So the reduction in the readout could be a mean or max, but not a sum. In contrast, an extensive label should (generally) use a sum for the reduction in the readout. An example of an extensive molecular property is enthalpy of formation.</p>
</section>
</section>
<section id="battaglia-general-equations">
<h2><span class="section-number">8.10. </span>Battaglia General Equations<a class="headerlink" href="#battaglia-general-equations" title="Permalink to this headline">¶</a></h2>
<p>As you can see, message passing layers is a general way to view GNN layers. Battaglia <em>et al.</em> <span id="id19">[<a class="reference internal" href="#id74" title="Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, and others. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.">BHB+18</a>]</span> went further and created a general set of equations which captures nearly all GNNs. They broke the GNN layer equations down into 3 update equations, like the node update equation we saw in the message passing layer equations, and 3 aggregation equations (6 total equations). There is a new concept in these equations: graph feature vectors. Instead of having two parts to your network (GNN then readout), a graph level feature is updated at every GNN layer. The graph feature vector is a set of features which represent the whole graph or molecule. For example, when computing solubility it may have been useful to build up a per-molecule feature vector that is eventually used to compute solubility instead of having the readout. Any kind of per-molecule quantity, like energy, should be predicted with the graph-level feature vector.</p>
<p>The first step in these equations is updating the edge feature vectors, written as <span class="math notranslate nohighlight">\(\vec{e}_k\)</span>, which we haven’t seen yet:</p>
<div class="math notranslate nohighlight" id="equation-edge-update">
<span class="eqno">(8.8)<a class="headerlink" href="#equation-edge-update" title="Permalink to this equation">¶</a></span>\[
\vec{e}^{'}_k = \phi^e\left( \vec{e}_k, \vec{v}_{rk}, \vec{v}_{sk}, \vec{u}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\vec{e}_k\)</span> is the feature vector of edge <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(\vec{v}_{rk}\)</span> is the receiving node feature vector for edge <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(\vec{v}_{sk}\)</span> is the sending node feature vector for edge <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(\vec{u}\)</span> is the graph feature vector, and <span class="math notranslate nohighlight">\(\phi^e\)</span> is one of the three update functions that the define the GNN layer. Note that these are meant to be general expressions and you define <span class="math notranslate nohighlight">\(\phi^e\)</span> for your specific GNN layer.</p>
<p>Our molecular graphs are undirected, so how do we decide which node is receiving <span class="math notranslate nohighlight">\(\vec{v}_{rk}\)</span> and which node is sending <span class="math notranslate nohighlight">\(\vec{v}_{sk}\)</span>? The individual <span class="math notranslate nohighlight">\(\vec{e}^{'}_k\)</span> are aggregated in the next step as all the inputs into node <span class="math notranslate nohighlight">\(v_{rk}\)</span>. In our molecular graph, all bonds are both “inputs” and “outputs” from an atom (how else could it be?), so it makes sense to just view every bond as two directed edges: a C-H bond has an edge from C to H and an edge from H to C. In fact, our adjacency matrices already reflect that. There are two non-zero elements in them for each bond: one for C to H and one for H to C. Back to the original question, what is <span class="math notranslate nohighlight">\(\vec{v}_{rk}\)</span> and <span class="math notranslate nohighlight">\(\vec{v}_{sk}\)</span>? We consider every element in the adjacency matrix (every <span class="math notranslate nohighlight">\(k\)</span>) and when we’re on element <span class="math notranslate nohighlight">\(k = \{ij\}\)</span>, which is <span class="math notranslate nohighlight">\(A_{ij}\)</span>, then the receiving node is <span class="math notranslate nohighlight">\(j\)</span> and the sending node is <span class="math notranslate nohighlight">\(i\)</span>. When we consider the companion edge <span class="math notranslate nohighlight">\(A_{ji}\)</span>, the receiving node is <span class="math notranslate nohighlight">\(i\)</span> and the sending node is <span class="math notranslate nohighlight">\(j\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\vec{e}^{'}_k\)</span> is like the message from the GCN. Except it’s more general: it can depend on the receiving node and the graph feature vector <span class="math notranslate nohighlight">\(\vec{u}\)</span>. The metaphor of a “message” doesn’t quite apply, since a message cannot be affected by the receiver. Anyway, the new edge updates are then aggregated with the first aggregation function:</p>
<div class="math notranslate nohighlight" id="equation-edge-aggregation">
<span class="eqno">(8.9)<a class="headerlink" href="#equation-edge-aggregation" title="Permalink to this equation">¶</a></span>\[
\bar{e}^{'}_i = \rho^{e\rightarrow v}\left( E_i^{'}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\rho^{e\rightarrow v}\)</span> is our defined function and <span class="math notranslate nohighlight">\(E_i^{'}\)</span> represents stacking all <span class="math notranslate nohighlight">\(\vec{e}^{'}_k\)</span> from edges  <strong>into</strong> node i. Having our aggregated edges, we can compute the node update:</p>
<div class="math notranslate nohighlight" id="equation-node-update">
<span class="eqno">(8.10)<a class="headerlink" href="#equation-node-update" title="Permalink to this equation">¶</a></span>\[
\vec{v}^{'}_i = \phi^v\left( \bar{e}^{'}_i, \vec{v}_i, \vec{u}\right)
\]</div>
<p>This concludes the usual steps of a GNN layer because we have new nodes and new edges. If you are updating the graph features (<span class="math notranslate nohighlight">\(\vec{u}\)</span>), the following additional steps may be defined:</p>
<div class="math notranslate nohighlight" id="equation-edge-all-aggregation">
<span class="eqno">(8.11)<a class="headerlink" href="#equation-edge-all-aggregation" title="Permalink to this equation">¶</a></span>\[
\bar{e}^{'} = \rho^{e\rightarrow u}\left( E^{'}\right)
\]</div>
<p>This equation aggregates all messages/aggregated edges across the whole graph. Then we can aggregate the new nodes across the whole graph:</p>
<div class="math notranslate nohighlight" id="equation-node-all-aggregation">
<span class="eqno">(8.12)<a class="headerlink" href="#equation-node-all-aggregation" title="Permalink to this equation">¶</a></span>\[
\bar{v}^{'} = \rho^{v\rightarrow u}\left( V^{'}\right)
\]</div>
<p>Finally, we can compute the update to the graph feature vector as:</p>
<div class="math notranslate nohighlight" id="equation-global-update">
<span class="eqno">(8.13)<a class="headerlink" href="#equation-global-update" title="Permalink to this equation">¶</a></span>\[
\vec{u}^{'} = \phi^u\left( \bar{e}^{'},\bar{v}^{'}, \vec{u}\right)
\]</div>
<section id="reformulating-gcn-into-battaglia-equations">
<h3><span class="section-number">8.10.1. </span>Reformulating GCN into Battaglia equations<a class="headerlink" href="#reformulating-gcn-into-battaglia-equations" title="Permalink to this headline">¶</a></h3>
<p>Let’s see how the GCN is presented in this form. We first compute our neighbor messages for all possible neighbors using <a class="reference internal" href="#equation-edge-update">(8.8)</a>. Remember in the GCN, messages only depend on the senders.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Even though we use the “edge update” function, remember in a GCN we ignore
the edge features. We only care edges for defining the connectivity of the graph.</p>
</aside>
<div class="math notranslate nohighlight">
\[
\vec{e}^{'}_k = \phi^e\left( \vec{e}_k, \vec{v}_{rk}, \vec{v}_{sk}, \vec{u}\right) = \vec{v}_{sk} \mathbf{W}
\]</div>
<p>To aggregate our messages coming into node <span class="math notranslate nohighlight">\(i\)</span> in <a class="reference internal" href="#equation-edge-aggregation">(8.9)</a>, we average them.</p>
<div class="math notranslate nohighlight">
\[
\bar{e}^{'}_i = \rho^{e\rightarrow v}\left( E_i^{'}\right) = \frac{1}{|E_i^{'}|}\sum E_i^{'}
\]</div>
<p>Our node update is then the activation <a class="reference internal" href="#equation-node-update">(8.10)</a></p>
<div class="math notranslate nohighlight">
\[
\vec{v}^{'}_i = \phi^v\left( \bar{e}^{'}_i, \vec{v}_i, \vec{u}\right) = \sigma(\bar{e}^{'}_i)
\]</div>
<p>we could include the self-loop above using <span class="math notranslate nohighlight">\(\sigma(\bar{e}^{'}_i + \vec{v}_i)\)</span>. The other functions are not used in a GCN, so those three completely define the GCN.</p>
</section>
</section>
<section id="the-schnet-architecture">
<h2><span class="section-number">8.11. </span>The SchNet Architecture<a class="headerlink" href="#the-schnet-architecture" title="Permalink to this headline">¶</a></h2>
<p>One of the earliest and most popular GNNs is the SchNet network <span id="id20">[<a class="reference internal" href="#id224" title="Kristof T Schütt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R Müller. Schnet–a deep learning architecture for molecules and materials. The Journal of Chemical Physics, 148(24):241722, 2018.">SchuttSK+18</a>]</span>. It wasn’t really recognized at publication time as a GNN, but its now recognized as one and you’ll see it often used as a baseline model. A <strong>baseline</strong> model is a well-accepted and accurate model that is compared with.</p>
<aside class="margin sidebar">
<p class="sidebar-title">Baseline Models</p>
<p>A common piece of wisdom is if you want to solve a real problem with deep learning, you should read the most recent popular paper in an area and use the baseline they compare against instead of their proposed model. The reason is that a baseline model usually must be easy, fast, and well-tested, which is generally more important than being the most accurate</p>
</aside>
<p>SchNet is for atoms represented as xyz coordinates (points) – not as a molecular graph. All our previous examples used the underlying molecular graph as the input. In SchNet we will convert our xyz coodinates into a graph, so that we can apply a GNNN. SchNet was developed for predicting energies and forces from atom configurations without bond information. Thus, we need to first see how a set of atoms and their positions is converted into a graph. To get the nodes, we do a similar process as above and the atomic number is passed through an embedding layer, which is just means we assign a trainable vector to each atomic number (See <a class="reference internal" href="layers.html"><span class="doc">Standard Layers</span></a> for a review of embeddings).</p>
<p>Getting the adjacency matrix is simple too: we just make every atom be connected to every atom. It might seem confusing what the point of using a GNN is, if we’re just connecting everything. <em>It is because GNNs are permutation equivariant.</em> If we tried to do learning on the atoms as xyz coordinates, we would have weights depending on the ordering of atoms and probably fail to handle different numbers of atoms.</p>
<p>There is one more missing detail: where do the xyz coordinates go? We make the model depend on xyz coordinates by constructing the edge features from the xyz coordinates. The edge <span class="math notranslate nohighlight">\(\vec{e}\)</span> between atoms <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> is computed purely from their distance <span class="math notranslate nohighlight">\(r\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-rbf-edge">
<span class="eqno">(8.14)<a class="headerlink" href="#equation-rbf-edge" title="Permalink to this equation">¶</a></span>\[
e_k = \exp\left(-\gamma \left(r - \mu_k\right)^2\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma\)</span> is a hyperparameter (e.g., 10Å) and <span class="math notranslate nohighlight">\(\mu_k\)</span> is an equally-space grid of scalars - like <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">5,</span> <span class="pre">10,</span> <span class="pre">15</span> <span class="pre">,</span> <span class="pre">20]</span></code>. The purpose of <a class="reference internal" href="#equation-rbf-edge">(8.14)</a> is similar to turning a category feature like atomic number or covalent bond type into a one-hot vector. We cannot do a one-hot vector though, because there is an infinite number of possible distances. Thus, we have a kind of “smoothing” that gives us a pseudo one-hot for distance. Let’s see an example to get a sense of it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gamma</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">rbf</span><span class="p">(</span><span class="n">r</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">rbf</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>input 2
output [0.02 0.78 0.   0.   0.  ]
</pre></div>
</div>
</div>
</div>
<p>You can see that a distance of <span class="math notranslate nohighlight">\(r=2\)</span> gives a vector with most of the activation for the <span class="math notranslate nohighlight">\(k = 1\)</span> position - which corresponds to <span class="math notranslate nohighlight">\(\mu_1 = 2\)</span>.</p>
<p>We have our nodes and edges and are close to defining the GNN update equations. We need a bit more notation. I’m going to use <span class="math notranslate nohighlight">\(h(\vec{x})\)</span> to indicate a multilayer perceptron (MLP) – basically a 1 to 2 dense layers neural network. The exact number of dense layers and when/where activation is used in these MLPs will be defined in the implementation, because it is not so important for understanding. Recall, the definition of a dense layer is</p>
<div class="math notranslate nohighlight">
\[
h(\vec{x}) = \sigma\left(Wx + b\right)
\]</div>
<p>We’ll also use a different activation function <span class="math notranslate nohighlight">\(\sigma\)</span> called “shifted softplus” in SchNet: <span class="math notranslate nohighlight">\(\ln\left(0.5e^{x} + 0.5\right)\)</span>. You can see <span class="math notranslate nohighlight">\(\sigma(x)\)</span> compared with the usual ReLU activation in <a class="reference internal" href="#softplus"><span class="std std-numref">Fig. 8.4</span></a>. The rationale for using shifted softplus is that it is smooth with-respect to its input, so it could be used to compute forces in a molecular dynamics simulation which requires taking smooth derivatives with respect to pairwise distances.</p>
<figure class="align-default" id="softplus">
<div class="cell_output docutils container">
<img alt="../_images/gnn_49_0.png" src="../_images/gnn_49_0.png" />
</div>
<figcaption>
<p><span class="caption-number">Fig. 8.4 </span><span class="caption-text">Comparison of the usual ReLU activation function and the shifted softplus used in the SchNet model.</span><a class="headerlink" href="#softplus" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Now, the GNN equations! The edge update equation <a class="reference internal" href="#equation-edge-update">(8.8)</a> is composed of two pieces. First, we run the incoming edge feature through an MLP and the atoms through an MLP. Then the result is run through an MLP:</p>
<div class="math notranslate nohighlight">
\[
\vec{e}^{'}_k = \phi^e\left( \vec{e}_k, \vec{v}_{rk}, \vec{v}_{sk}, \vec{u}\right) =h_1\left(\vec{v}_{sk}\right) \cdot h_2\left(\vec{e}_k\right)
\]</div>
<p>The next equation is the edge aggregation equation, <a class="reference internal" href="#equation-edge-aggregation">(8.9)</a>. For SchNet, the edge aggregation is a sum over the neighbor atom features.</p>
<div class="math notranslate nohighlight">
\[
\bar{e}^{'}_i = \sum E_i^{'}
\]</div>
<p>Finally, the node update equation for SchNet is:</p>
<div class="math notranslate nohighlight">
\[
\vec{v}^{'}_i = \phi^v\left( \bar{e}^{'}_i, \vec{v}_i, \vec{u}\right) = \vec{v}_i + h_3\left(\bar{e}^{'}_i\right)
\]</div>
<p>The GNN updates are applied typically 3-6 times. Although we have an edge update equation, like in GCN we do not actually override the edges and keep them the same at each layer. The original SchNet was for predicting energies and forces, so a readout can be done using sum-pooling or any other strategy described above.</p>
<p>These are sometimes changed, but in the original SchNet paper <span class="math notranslate nohighlight">\(h_1\)</span> is one dense layer without activation, <span class="math notranslate nohighlight">\(h_2\)</span> is two dense layers with activation, and <span class="math notranslate nohighlight">\(h_3\)</span> is 2 dense layers with activation on the first and not the second.</p>
<div class="admonition-what-is-schnet admonition">
<p class="admonition-title">What is SchNet?</p>
<p>The key GNN feature of a SchNet-like GNN are (1) use edge &amp; node features in the edge update (message construction):</p>
<div class="math notranslate nohighlight">
\[
\vec{e}^{'}_k = h_1(\vec{v}_{sk}) \cdot h_2(\vec{e}_k)
\]</div>
<p>where <span class="math notranslate nohighlight">\(h_i()\)</span>s are some trainable functions and (2) use a residue in the node update:</p>
<div class="math notranslate nohighlight">
\[
\vec{v}^{'}_i = \vec{v}_i + h_3\left(\bar{e}^{'}_i\right)
\]</div>
</div>
<p>All the other details about how to featurize the edges, how deep <span class="math notranslate nohighlight">\(h_i\)</span> is, what activation to choose, how to readout, and how to convert point clouds to graphs are about the specific SchNet model in <span id="id21">[<a class="reference internal" href="#id224" title="Kristof T Schütt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R Müller. Schnet–a deep learning architecture for molecules and materials. The Journal of Chemical Physics, 148(24):241722, 2018.">SchuttSK+18</a>]</span>.</p>
</section>
<section id="schnet-example-predicting-space-groups">
<h2><span class="section-number">8.12. </span>SchNet Example: Predicting Space Groups<a class="headerlink" href="#schnet-example-predicting-space-groups" title="Permalink to this headline">¶</a></h2>
<p>Our next example will be a SchNet model that predict space groups of points. Identifying the space group of atoms is an important part of crystal structure identification, and when doing simulations of crystallization. Our SchNet model will take as input points and output the predicted space group. This is a classification problem; specifically it is multi-class becase a set of points should only be in one space group. To simplify our plots and analysis, we will work in 2D where there are 17 possible space groups.</p>
<p>Our data for this is a set of points from various point groups. The features are xyz coordinates and the label is the space group. We will not have multiple atom types for this problem. The hidden cell below loads the data and reshapes it for the example.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gzip</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">urllib</span>

<span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span>
    <span class="s2">&quot;https://github.com/whitead/dmol-book/raw/master/data/sym_trajs.pb.gz&quot;</span><span class="p">,</span>
    <span class="s2">&quot;sym_trajs.pb.gz&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">gzip</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;sym_trajs.pb.gz&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">trajs</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="n">label_str</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">([</span><span class="n">k</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">trajs</span><span class="p">]))</span>

<span class="c1"># now build dataset</span>
<span class="k">def</span> <span class="nf">generator</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">trajs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">ls</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">label_str</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">ls</span><span class="p">)</span>
        <span class="n">traj</span> <span class="o">=</span> <span class="n">v</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">traj</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">yield</span> <span class="n">traj</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span>


<span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_generator</span><span class="p">(</span>
    <span class="n">generator</span><span class="p">,</span>
    <span class="n">output_signature</span><span class="o">=</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
    <span class="p">),</span>
<span class="p">)</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span>
    <span class="mi">1000</span><span class="p">,</span>
    <span class="n">reshuffle_each_iteration</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># do not change order each time (!) otherwise will contaminate</span>
<span class="p">)</span>

<span class="c1"># The shuffling above is really important because this dataset is in order of labels!</span>

<span class="n">val_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s take a look at a few examples from the dataset</p>
<div class="dropdown admonition">
<p class="admonition-title">The Data</p>
<p>This data was generated from <span id="id22">[<a class="reference internal" href="#id225" title="Sam Cox and Andrew D White. Symmetric molecular dynamics. arXiv preprint arXiv:2204.01114, 2022.">CW22</a>]</span> and all points are constrained to match the space group exactly during a molecular dynamics simulation. The trajectories were NPT with a positive pressure and followed the procedure in that paper for Figure 2. The force field is Lennard-Jones with <span class="math notranslate nohighlight">\(\sigma=1\)</span> and <span class="math notranslate nohighlight">\(\epsilon=1\)</span></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="c1"># get a few example and plot them</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">20</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">label_str</span><span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">()])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gnn_55_0.png" src="../_images/gnn_55_0.png" />
</div>
</div>
<p>You can see that there is a variable number of points and a few examples for each space group. The goal is to infer those titles on the plot from the points alone.</p>
<section id="building-the-graphs">
<h3><span class="section-number">8.12.1. </span>Building the graphs<a class="headerlink" href="#building-the-graphs" title="Permalink to this headline">¶</a></h3>
<p>We now need to build the graphs for the points. The nodes are all identical - so they can just be 1s (we’ll reserve 0 in case we want to mask or pad at some point in the future). As described in the SchNet section above, the edges should be distance to every other atom. In most implementations of SchNet, we practically add a cut-off on either distance or maximum degree (edges per node). We’ll do maximum degree for this work of 16.</p>
<p>I have a function below that is a bit sophisticated. It takes a matrix of point positions in arbitrary dimension and returns the distances and indices to the nearest <code class="docutils literal notranslate"><span class="pre">k</span></code> neighbors - exactly what we need. It uses some tricks from <a class="reference internal" href="../math/tensors-and-shapes.html"><span class="doc">Tensors and Shapes</span></a>. However, it is not so important for you to understand this function. Just know it takes in points and gives us the edge features and edge nodes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># this decorator speeds up the function by &quot;compiling&quot; it (tracing it)</span>
<span class="c1"># to run efficienty</span>
<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
    <span class="n">reduce_retracing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">get_edges</span><span class="p">(</span><span class="n">positions</span><span class="p">,</span> <span class="n">NN</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">positions</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># adjust NN</span>
    <span class="n">NN</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">NN</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
    <span class="n">qexpand</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">positions</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># one column</span>
    <span class="n">qTexpand</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">positions</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># one row</span>
    <span class="c1"># repeat it to make matrix of all positions</span>
    <span class="n">qtile</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">qexpand</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">qTtile</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">qTexpand</span><span class="p">,</span> <span class="p">[</span><span class="n">M</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="c1"># subtract them to get distance matrix</span>
    <span class="n">dist_mat</span> <span class="o">=</span> <span class="n">qTtile</span> <span class="o">-</span> <span class="n">qtile</span>
    <span class="c1"># mask distance matrix to remove zros (self-interactions)</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">dist_mat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">dist</span> <span class="o">&gt;=</span> <span class="mf">5e-4</span>
    <span class="n">mask_cast</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="c1"># make masked things be really far</span>
    <span class="n">dist_mat_r</span> <span class="o">=</span> <span class="n">dist</span> <span class="o">*</span> <span class="n">mask_cast</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mask_cast</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>
    <span class="n">topk</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">top_k</span><span class="p">(</span><span class="o">-</span><span class="n">dist_mat_r</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">NN</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="nb">sorted</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">topk</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">topk</span><span class="o">.</span><span class="n">indices</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s see how this function works by showing the connections between points in one of our examples. I’ve hidden the code below. It shows some point’s neighbors and connects them so you can get a sense of how a set of points is converted into a graph. The complete graph will have all points’ neighborhoods.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">collections</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">6</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="n">e_f</span><span class="p">,</span> <span class="n">e_i</span> <span class="o">=</span> <span class="n">get_edges</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>

    <span class="c1"># make things easier for plotting</span>
    <span class="n">e_i</span> <span class="o">=</span> <span class="n">e_i</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># make lines from origin to its neigbhors</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">23</span><span class="p">):</span>
        <span class="c1"># lines are [(xstart, ystart), (xend, yend)]</span>
        <span class="n">lines</span><span class="o">.</span><span class="n">extend</span><span class="p">([[(</span><span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">])]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">e_i</span><span class="p">[</span><span class="n">j</span><span class="p">]])</span>
        <span class="n">colors</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;C</span><span class="si">{</span><span class="n">j</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">e_i</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
    <span class="n">lc</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">LineCollection</span><span class="p">(</span><span class="n">lines</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">add_collection</span><span class="p">(</span><span class="n">lc</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">label_str</span><span class="p">[</span><span class="n">y</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gnn_59_0.png" src="../_images/gnn_59_0.png" />
</div>
</div>
<p>We will now add this function and the edge featurization of SchNet <a class="reference internal" href="#equation-rbf-edge">(8.14)</a> to get the graphs for the GNN steps.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">MAX_DEGREE</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">EDGE_FEATURES</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">MAX_R</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">gamma</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">MAX_R</span><span class="p">,</span> <span class="n">EDGE_FEATURES</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">rbf</span><span class="p">(</span><span class="n">r</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">make_graph</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">edge_r</span><span class="p">,</span> <span class="n">edge_i</span> <span class="o">=</span> <span class="n">get_edges</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">MAX_DEGREE</span><span class="p">)</span>
    <span class="n">edge_features</span> <span class="o">=</span> <span class="n">rbf</span><span class="p">(</span><span class="n">edge_r</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">edge_features</span><span class="p">,</span> <span class="n">edge_i</span><span class="p">),</span> <span class="n">y</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span>


<span class="n">graph_train_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">make_graph</span><span class="p">)</span>
<span class="n">graph_val_data</span> <span class="o">=</span> <span class="n">val_data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">make_graph</span><span class="p">)</span>
<span class="n">graph_test_data</span> <span class="o">=</span> <span class="n">test_data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">make_graph</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s examine one graph to see what it looks like. We’ll slice out only the first nodes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">nn</span><span class="p">),</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">graph_train_data</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;first node:&quot;</span><span class="p">,</span> <span class="n">n</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;first node, first edge features:&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;first node, all neighbors&quot;</span><span class="p">,</span> <span class="n">nn</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="k">break</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>first node: 1
first node, first edge features: [3.0456832e-01 4.4088218e-02 5.1812826e-10 4.9434674e-25 0.0000000e+00
 0.0000000e+00 0.0000000e+00 0.0000000e+00]
first node, all neighbors [213  12 226 129 229  17 222 130 205 230 225 134 149   5 209 119]
label [11]
</pre></div>
</div>
</div>
</div>
</section>
<section id="implementing-the-mlps">
<h3><span class="section-number">8.12.2. </span>Implementing the MLPs<a class="headerlink" href="#implementing-the-mlps" title="Permalink to this headline">¶</a></h3>
<p>Now we can implement the SchNet model! Let’s start with the <span class="math notranslate nohighlight">\(h_1,h_2,h_3\)</span> MLPs that are used in the GNN update equations. In the SchNet paper these each had different numbers of layers and different decisions about which layers had activation. Let’s create them now.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ssp</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># shifted softplus activation</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">make_h1</span><span class="p">(</span><span class="n">units</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">)])</span>


<span class="k">def</span> <span class="nf">make_h2</span><span class="p">(</span><span class="n">units</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">ssp</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">ssp</span><span class="p">),</span>
        <span class="p">]</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">make_h3</span><span class="p">(</span><span class="n">units</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">ssp</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">)]</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>One detail that can be missed is that the weights in each MLP should change in each layer of SchNet. Thus, we’ve written the functions above to always return a new MLP. This means that a new set of trainable weights is generated on each call, meaning there is no way we could erroneously have the same weights in multiple layers.</p>
</section>
<section id="implementing-the-gnn">
<h3><span class="section-number">8.12.3. </span>Implementing the GNN<a class="headerlink" href="#implementing-the-gnn" title="Permalink to this headline">¶</a></h3>
<p>Now we have all the pieces to make the GNN. This code will be very similar to the GCN example above, except we now have edge features. One more detail is that our readout will be an MLP as well, following the SchNet paper. The only change we’ll make is that we want our output property to be (1) multi-class classification and (2) intensive (independent of number of atoms). So we’ll end with an average (intensive) and end with an output vector of logits the size of our labels.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SchNetModel</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Implementation of SchNet Model&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gnn_blocks</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">label_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SchNetModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gnn_blocks</span> <span class="o">=</span> <span class="n">gnn_blocks</span>

        <span class="c1"># build our layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h1s</span> <span class="o">=</span> <span class="p">[</span><span class="n">make_h1</span><span class="p">(</span><span class="n">channels</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnn_blocks</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h2s</span> <span class="o">=</span> <span class="p">[</span><span class="n">make_h2</span><span class="p">(</span><span class="n">channels</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnn_blocks</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h3s</span> <span class="o">=</span> <span class="p">[</span><span class="n">make_h3</span><span class="p">(</span><span class="n">channels</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnn_blocks</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">readout_l1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">ssp</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">readout_l2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">label_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">nodes</span><span class="p">,</span> <span class="n">edge_features</span><span class="p">,</span> <span class="n">edge_i</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="c1"># turn node types as index to features</span>
        <span class="n">nodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnn_blocks</span><span class="p">):</span>
            <span class="c1"># get the node features per edge</span>
            <span class="n">v_sk</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="n">edge_i</span><span class="p">)</span>
            <span class="n">e_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h1s</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">v_sk</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">h2s</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">edge_features</span><span class="p">)</span>
            <span class="n">e_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">e_k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">nodes</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h3s</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">e_i</span><span class="p">)</span>
        <span class="c1"># readout now</span>
        <span class="n">nodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">readout_l1</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>
        <span class="n">nodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">readout_l2</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Remember that the key attributes of a SchNet GNN are the way that we use edge and node features. We can see the mixing of these two in the key line for computing the edge update (computing message values):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">e_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h1s</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">v_sk</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">h2s</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">edge_features</span><span class="p">)</span>
</pre></div>
</div>
<p>followed by aggregation of the edges updates (pooling messages):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">e_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">e_k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>and the node update</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nodes</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h3s</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">e_i</span><span class="p">)</span>
</pre></div>
</div>
<p>Also of note is how we go from node features to multi-classs. We use dense layers that get the shape per-node into the number of classes</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">readout_l1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">ssp</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">readout_l2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">label_dim</span><span class="p">)</span>
</pre></div>
</div>
<p>and then we take the average over all nodes</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
<p>Let’s give now use the model on some data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">small_schnet</span> <span class="o">=</span> <span class="n">SchNetModel</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">label_str</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">graph_train_data</span><span class="p">:</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">small_schnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">break</span>
<span class="nb">print</span><span class="p">(</span><span class="n">yhat</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 2.3440402e-02  1.2207472e-02 -9.7504584e-03 -1.1969811e-02
  8.9555839e-03  1.2503829e-02  3.1677008e-02  2.0824309e-02
 -2.8484619e-05  6.2389118e-03 -1.7091008e-02 -1.5034138e-02
  4.9161413e-03 -3.8988986e-03 -1.3190225e-02  1.9660493e-02
  1.0113629e-02]
</pre></div>
</div>
</div>
</div>
<p>The output is the correct shape and remember it is logits. To get a class prediction that sums to probability 1, we need to use a softmax:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;predicted class&quot;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>predicted class [0.05993128 0.05926184 0.05797476 0.05784623 0.05906944 0.0592794
 0.06042695 0.0597747  0.05854113 0.05890919 0.05755074 0.05766924
 0.05883131 0.05831499 0.05777568 0.05970517 0.05913788]
</pre></div>
</div>
</div>
</div>
</section>
<section id="training">
<h3><span class="section-number">8.12.4. </span>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h3>
<p>Great! It is untrained though. Now we can set-up training. Our loss will be cross-entropy from logits, but we need to be careful on the form. Our labels are integers - which is called “sparse” labels because they are not full one-hots. Mult-class classification is also known as categorical classification. Thus, the loss we want is sparse categorical cross entropy from logits.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">small_schnet</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">),</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">metrics</span><span class="o">=</span><span class="s2">&quot;sparse_categorical_accuracy&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">small_schnet</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">graph_train_data</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">graph_val_data</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;sparse_categorical_accuracy&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;val_sparse_categorical_accuracy&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;validation accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">17</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;random&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gnn_75_0.png" src="../_images/gnn_75_0.png" />
</div>
</div>
<p>The accuracy is not great, but it looks like we could keep training. We have a very small SchNet here. Standard SchNet described in <span id="id23">[<a class="reference internal" href="#id224" title="Kristof T Schütt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R Müller. Schnet–a deep learning architecture for molecules and materials. The Journal of Chemical Physics, 148(24):241722, 2018.">SchuttSK+18</a>]</span> uses 6 layers and 64 channels and 300 edge features. We have 3 layers and 32 channels. Nevertheless, we’re able to get some learning. Let’s visually see what’s going on with the trained model on some test data</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="p">(</span><span class="n">gx</span><span class="p">,</span> <span class="n">_</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">graph_test_data</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">20</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">small_schnet</span><span class="p">(</span><span class="n">gx</span><span class="p">)</span>
    <span class="n">yhat_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">yhat</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True: </span><span class="si">{</span><span class="n">label_str</span><span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span><span class="si">}</span><span class="se">\n</span><span class="s2">Predicted: </span><span class="si">{</span><span class="n">label_str</span><span class="p">[</span><span class="n">yhat_i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gnn_77_0.png" src="../_images/gnn_77_0.png" />
</div>
</div>
<p>We’ll revisit this example later! One unique fact about this dataset is that it is <em>synthetic</em>, meaning there is no label noise. As discussed in <a class="reference internal" href="../ml/regression.html"><span class="doc">Regression &amp; Model Assessment</span></a>, that removes the possibility of overfitting and leads us to favor high variance models. The goal of teaching a model to predict space groups is to apply it on real simulations or microscopy data, which will certainly have noise. We could have mimicked this by adding noise to the labels in the data and/or by randomly removing atoms to simulate defects. This would better help our model work in a real setting.</p>
</section>
</section>
<section id="current-research-directions">
<h2><span class="section-number">8.13. </span>Current Research Directions<a class="headerlink" href="#current-research-directions" title="Permalink to this headline">¶</a></h2>
<section id="common-architecture-motifs-and-comparisons">
<h3><span class="section-number">8.13.1. </span>Common Architecture Motifs and Comparisons<a class="headerlink" href="#common-architecture-motifs-and-comparisons" title="Permalink to this headline">¶</a></h3>
<p>We’ve now seen message passing layer GNNs, GCNs, GGNs, and the generalized Battaglia equations. You’ll find common motifs in the architectures, like gating, <a class="reference internal" href="attention.html"><span class="doc">Attention Layers</span></a>, and pooling strategies. For example, Gated GNNS (GGNs) can be combined with attention pooling to create Gated Attention GNNs (GAANs)<span id="id24">[<a class="reference internal" href="#id80" title="Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: gated attention networks for learning on large and spatiotemporal graphs. arXiv preprint arXiv:1803.07294, 2018.">ZSX+18</a>]</span>. GraphSAGE is a similar to a GCN but it samples when pooling, making the neighbor-updates of fixed dimension<span id="id25">[<a class="reference internal" href="#id91" title="Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in neural information processing systems, 1024–1034. 2017.">HYL17</a>]</span>. So you’ll see the suffix “sage” when you sample over neighbors while pooling. These can all be represented in the Battaglia equations, but you should be aware of these names.</p>
<p>The enormous variety of architectures has led to work on identifying the “best” or most general GNN architecture <span id="id26">[<a class="reference internal" href="#id94" title="Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020.">DJL+20</a>, <a class="reference internal" href="#id93" title="Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. A fair comparison of graph neural networks for graph classification. In International Conference on Learning Representations. 2019.">EPBM19</a>, <a class="reference internal" href="#id92" title="Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.">SMBGunnemann18</a>]</span>. Unfortunately, the question of which GNN architecture is best is as difficult as “what benchmark problems are best?” Thus there are no agreed-upon conclusions on the best architecture. However, those papers are great resources on training, hyperparameters, and reasonable starting guesses and I highly recommend reading them before designing your own GNN. There has been some theoretical work to show that simple architectures, like GCNs, cannot distinguish between certain simple graphs <span id="id27">[<a class="reference internal" href="#id90" title="Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations. 2018.">XHLJ18</a>]</span>. How much this practically matters depends on your data. Ultimately, there is so much variety in hyperparameters, data equivariances, and training decisions that you should think carefully about how much the GNN architecture matters before exploring it with too much depth.</p>
</section>
<section id="nodes-edges-and-features">
<h3><span class="section-number">8.13.2. </span>Nodes, Edges, and Features<a class="headerlink" href="#nodes-edges-and-features" title="Permalink to this headline">¶</a></h3>
<p>You’ll find that most GNNs use the node-update equation in the Battaglia equations but do not update edges. For example, the GCN will update nodes at each layer but the edges are constant. Some recent work has shown that updating edges can be important for learning when the edges have geometric information, like if the input graph is a molecule and the edges are distance between the atoms <span id="id28">[<a class="reference internal" href="#id106" title="Johannes Klicpera, Janek Groß, and Stephan Günnemann. Directional message passing for molecular graphs. In International Conference on Learning Representations. 2020.">KGrossGunnemann20</a>]</span>. As we’ll see in the chapter on equivariances (<a class="reference internal" href="data.html"><span class="doc">Input Data &amp; Equivariances</span></a>), one of the key properties of neural networks with point clouds (i.e., Cartesian xyz coordinates) is to have rotation equivariance. <span id="id29">[<a class="reference internal" href="#id106" title="Johannes Klicpera, Janek Groß, and Stephan Günnemann. Directional message passing for molecular graphs. In International Conference on Learning Representations. 2020.">KGrossGunnemann20</a>]</span> showed that you can achieve this if you do edge updates and encode the edge vectors using a rotation equivariant basis set with spherical harmonics and Bessel functions. These kind of edge updating GNNs can be used to predict protein structure <span id="id30">[<a class="reference internal" href="#id107" title="Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael JL Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. arXiv preprint arXiv:2009.01411, 2020.">JES+20</a>]</span>.</p>
<p>Another common variation on node features is to pack more into node features than just element identity. In many examples, you will see people inserting valence, elemental mass, electronegativity, a bit indicating if the atom is in a ring, a bit indicating if the atom is aromatic, etc. Typically these are unnecessary, since a model should be able to learn any of these features which are computed from the graph and node elements. However, we and others have empirically found that some can help, specifically indicating if an atom is in a ring <span id="id31">[<a class="reference internal" href="#id108" title="Zhiheng Li, Geemi P Wellawatte, Maghesree Chakraborty, Heta A Gandhi, Chenliang Xu, and Andrew D White. Graph neural network based coarse-grained mapping prediction. Chemical Science, 11(35):9524–9531, 2020.">LWC+20</a>]</span>. Choosing extra features to include though should be at the bottom of your list of things to explore when designing and using GNNs.</p>
</section>
<section id="beyond-message-passing">
<h3><span class="section-number">8.13.3. </span>Beyond Message Passing<a class="headerlink" href="#beyond-message-passing" title="Permalink to this headline">¶</a></h3>
<p>One of the common themes of GNN research is moving “beyond message passing,” where message passing is the message construction, aggregation, and node update with messages. Some view this as impossible – claiming that all GNNs can be recast as message passing <span id="id32">[<a class="reference internal" href="#id226" title="Petar Veličković. Message passing all the way up. arXiv preprint arXiv:2202.11097, 2022.">Velivckovic22</a>]</span>. Another direction is on disconnecting the underlying graph being input to the GNN and the graph used to compute updates. We sort of saw this above with SchNet, where we restricted the maximum degree for the message passing. More useful are ideas like “lifting” the graphs into more structured objects like simplicial complexes <span id="id33">[<a class="reference internal" href="#id228" title="Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido F Montufar, and Michael Bronstein. Weisfeiler and lehman go cellular: cw networks. Advances in Neural Information Processing Systems, 34:2625–2640, 2021.">BFO+21</a>]</span>. Finally, you can also choose where to send the messages beyond just neighbors <span id="id34">[<a class="reference internal" href="#id227" title="Erik Thiede, Wenda Zhou, and Risi Kondor. Autobahn: automorphism-based graph neural nets. Advances in Neural Information Processing Systems, 2021.">TZK21</a>]</span>. For example, all nodes on a path could communicate messages or all nodes in a clique.</p>
</section>
<section id="do-we-need-graphs">
<h3><span class="section-number">8.13.4. </span>Do we need graphs?<a class="headerlink" href="#do-we-need-graphs" title="Permalink to this headline">¶</a></h3>
<p>It is possible to convert a graph into a string if you’re working with an adjacency matrix without continuous values. Molecules specifically can be converted into a string. This means you can use layers for sequences/strings (e.g., recurrent neural networks or 1D convolutions) and avoid the complexities of a graph neural network. SMILES is one way to convert molecular graphs into strings. With SMILES, you cannot predict a per-atom quantity and thus a graph neural network is required for atom/bond labels. However, the choice is less clear for per-molecule properties like toxicity or solubility. There is no consensus about if a graph or string/SMILES representation is better. SMILES can exceed certain graph neural networks in accuracy on some tasks. SMILES is typically better on generative tasks. Graphs obviously beat SMILES in label representations, because they have granularity of bonds/edges.  We’ll see how to model SMILES in <a class="reference internal" href="NLP.html"><span class="doc">Deep Learning on Sequences</span></a>, but it is an open question of which is better.</p>
</section>
<section id="stereochemistry-chiral-molecules">
<h3><span class="section-number">8.13.5. </span>Stereochemistry/Chiral Molecules<a class="headerlink" href="#stereochemistry-chiral-molecules" title="Permalink to this headline">¶</a></h3>
<p>Stereochemistry is fundamentally a 3D property of molecules and thus not present in the covalent bonding. It is measured experimentally by seeing if molecules rotate polarized light and a molecule is called chiral or “optically active” if it is experimentally known to have this property. Stereochemistry is the categorization of how molecules can preferentially rotate polarized light through asymmetries with respect to their mirror images. In organic chemistry, the majority of stereochemistry is of enantiomers. Enantiomers are “handedness” around specific atoms called chiral centers which have 4 or more different bonded atoms. These may be treated in a graph by indicating which nodes are chiral centers (nodes) and what their state or mixture of states (racemic) are. This can be treated as an extra processing step. Amino acids and thus all proteins are entaniomers with only one form present. This chirality of proteins means many drug molecules can be more or less potent depending on their stereochemistry.</p>
<figure class="align-default" id="helicene">
<a class="autoplay-video reference internal image-reference" href="../_images/helicene.mp4"><video class="autoplay-video" src="../_images/helicene.mp4" title="../_images/helicene.mp4" width="500"><a href="../_images/helicene.mp4">../_images/helicene.mp4</a></video></a>
<figcaption>
<p><span class="caption-number">Fig. 8.5 </span><span class="caption-text">This is a molecule with axial stereochemistry. Its small helix could be either left or right-handed.</span><a class="headerlink" href="#helicene" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Adding node labels is not enough generally. Molecules can interconvert between stereoisomers at chiral centers through a process called tautomerization. There are also types of stereochemistry that are not at a specific atom, like rotamers that are around a bond. Then there is stereochemistry that involves multiple atoms like axial helecene. As shown in <a class="reference internal" href="#helicene"><span class="std std-numref">Fig. 8.5</span></a>, the molecule has no chiral centers but is “optically active” (experimentally measured to be chiral) because of its helix which can be left- or right-handed.</p>
</section>
</section>
<section id="relevant-videos">
<h2><span class="section-number">8.14. </span>Relevant Videos<a class="headerlink" href="#relevant-videos" title="Permalink to this headline">¶</a></h2>
<section id="intro-to-gnns">
<h3><span class="section-number">8.14.1. </span>Intro to GNNs<a class="headerlink" href="#intro-to-gnns" title="Permalink to this headline">¶</a></h3>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/uF53xsT7mjc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</section>
<section id="overview-of-gnn-with-molecule-compiler-examples">
<h3><span class="section-number">8.14.2. </span>Overview of GNN with Molecule, Compiler Examples<a class="headerlink" href="#overview-of-gnn-with-molecule-compiler-examples" title="Permalink to this headline">¶</a></h3>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/zCEYiCxrL_0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</section>
</section>
<section id="chapter-summary">
<h2><span class="section-number">8.15. </span>Chapter Summary<a class="headerlink" href="#chapter-summary" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Molecules can be represented by graphs by using one-hot encoded feature vectors that show the elemental identity of each node (atom) and an adjacency matrix that show immediate neighbors (bonded atoms).</p></li>
<li><p>Graph neural networks are a category of deep neural networks that have graphs as inputs.</p></li>
<li><p>One of the early GNNs is the Kipf &amp; Welling GCN. The input to the GCN is the node feature vector and the adjacency matrix, and returns the updated node feature vector. The GCN is permutation invariant because it averages over the neighbors.</p></li>
<li><p>A GCN can be viewed as a message-passing layer, in which we have senders and receivers. Messages are computed from neighboring nodes, which when aggregated update that node.</p></li>
<li><p>A gated graph neural network is a variant of the message passing layer, for which the nodes are updated according to a gated recurrent unit function.</p></li>
<li><p>The aggregation of messages is sometimes called pooling, for which there are multiple reduction operations.</p></li>
<li><p>GNNs output a graph. To get a per-atom or per-molecule property, use a readout function. The readout depends on if your property is intensive vs extensive</p></li>
<li><p>The Battaglia equations encompasses almost all GNNs into a set of 6 update and aggregation equations.</p></li>
<li><p>You can convert xyz coordinates into a graph and use a GNN like SchNet</p></li>
</ul>
</section>
<section id="cited-references">
<h2><span class="section-number">8.16. </span>Cited References<a class="headerlink" href="#cited-references" title="Permalink to this headline">¶</a></h2>
<div class="docutils container" id="id35">
<dl class="citation">
<dt class="label" id="id94"><span class="brackets">DJL+20</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id26">2</a>)</span></dt>
<dd><p>Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. <em>arXiv preprint arXiv:2003.00982</em>, 2020.</p>
</dd>
<dt class="label" id="id95"><span class="brackets"><a class="fn-backref" href="#id2">BBL+17</a></span></dt>
<dd><p>Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. <em>IEEE Signal Processing Magazine</em>, 34(4):18–42, 2017.</p>
</dd>
<dt class="label" id="id96"><span class="brackets"><a class="fn-backref" href="#id3">WPC+20</a></span></dt>
<dd><p>Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. <em>IEEE Transactions on Neural Networks and Learning Systems</em>, 2020.</p>
</dd>
<dt class="label" id="id108"><span class="brackets">LWC+20</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id31">2</a>)</span></dt>
<dd><p>Zhiheng Li, Geemi P Wellawatte, Maghesree Chakraborty, Heta A Gandhi, Chenliang Xu, and Andrew D White. Graph neural network based coarse-grained mapping prediction. <em>Chemical Science</em>, 11(35):9524–9531, 2020.</p>
</dd>
<dt class="label" id="id109"><span class="brackets"><a class="fn-backref" href="#id5">YCW20</a></span></dt>
<dd><p>Ziyue Yang, Maghesree Chakraborty, and Andrew D White. Predicting chemical shifts with graph neural networks. <em>bioRxiv</em>, 2020.</p>
</dd>
<dt class="label" id="id56"><span class="brackets"><a class="fn-backref" href="#id6">XFLW+19</a></span></dt>
<dd><p>Tian Xie, Arthur France-Lanord, Yanming Wang, Yang Shao-Horn, and Jeffrey C Grossman. Graph dynamical networks for unsupervised learning of atomic scale dynamics in materials. <em>Nature communications</em>, 10(1):1–9, 2019.</p>
</dd>
<dt class="label" id="id192"><span class="brackets"><a class="fn-backref" href="#id7">SLRPW21</a></span></dt>
<dd><p>Benjamin Sanchez-Lengeling, Emily Reif, Adam Pearce, and Alex Wiltschko. A gentle introduction to graph neural networks. <em>Distill</em>, 2021. https://distill.pub/2021/gnn-intro. <a class="reference external" href="https://doi.org/10.23915/distill.00033">doi:10.23915/distill.00033</a>.</p>
</dd>
<dt class="label" id="id132"><span class="brackets"><a class="fn-backref" href="#id8">XG18</a></span></dt>
<dd><p>Tian Xie and Jeffrey C. Grossman. Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. <em>Phys. Rev. Lett.</em>, 120:145301, Apr 2018. URL: <a class="reference external" href="https://link.aps.org/doi/10.1103/PhysRevLett.120.145301">https://link.aps.org/doi/10.1103/PhysRevLett.120.145301</a>, <a class="reference external" href="https://doi.org/10.1103/PhysRevLett.120.145301">doi:10.1103/PhysRevLett.120.145301</a>.</p>
</dd>
<dt class="label" id="id73"><span class="brackets"><a class="fn-backref" href="#id9">KW16</a></span></dt>
<dd><p>Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. <em>arXiv preprint arXiv:1609.02907</em>, 2016.</p>
</dd>
<dt class="label" id="id77"><span class="brackets"><a class="fn-backref" href="#id10">GSR+17</a></span></dt>
<dd><p>Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. <em>arXiv preprint arXiv:1704.01212</em>, 2017.</p>
</dd>
<dt class="label" id="id75"><span class="brackets"><a class="fn-backref" href="#id11">LTBZ15</a></span></dt>
<dd><p>Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. <em>arXiv preprint arXiv:1511.05493</em>, 2015.</p>
</dd>
<dt class="label" id="id76"><span class="brackets"><a class="fn-backref" href="#id12">CGCB14</a></span></dt>
<dd><p>Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. <em>arXiv preprint arXiv:1412.3555</em>, 2014.</p>
</dd>
<dt class="label" id="id90"><span class="brackets">XHLJ18</span><span class="fn-backref">(<a href="#id13">1</a>,<a href="#id27">2</a>)</span></dt>
<dd><p>Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In <em>International Conference on Learning Representations</em>. 2018.</p>
</dd>
<dt class="label" id="id89"><span class="brackets"><a class="fn-backref" href="#id14">LDLio19</a></span></dt>
<dd><p>Enxhell Luzhnica, Ben Day, and Pietro Liò. On graph classification networks, datasets and baselines. <em>arXiv preprint arXiv:1905.04682</em>, 2019.</p>
</dd>
<dt class="label" id="id88"><span class="brackets"><a class="fn-backref" href="#id14">MSK20</a></span></dt>
<dd><p>Diego Mesquita, Amauri Souza, and Samuel Kaski. Rethinking pooling in graph neural networks. <em>Advances in Neural Information Processing Systems</em>, 2020.</p>
</dd>
<dt class="label" id="id196"><span class="brackets"><a class="fn-backref" href="#id15">GZBA21</a></span></dt>
<dd><p>Daniele Grattarola, Daniele Zambon, Filippo Maria Bianchi, and Cesare Alippi. Understanding pooling in graph neural networks. <em>arXiv preprint arXiv:2110.05292</em>, 2021.</p>
</dd>
<dt class="label" id="id198"><span class="brackets"><a class="fn-backref" href="#id16">DRA21</a></span></dt>
<dd><p>Ameya Daigavane, Balaraman Ravindran, and Gaurav Aggarwal. Understanding convolutions on graphs. <em>Distill</em>, 2021. https://distill.pub/2021/understanding-gnns. <a class="reference external" href="https://doi.org/10.23915/distill.00032">doi:10.23915/distill.00032</a>.</p>
</dd>
<dt class="label" id="id115"><span class="brackets">ZKR+17</span><span class="fn-backref">(<a href="#id17">1</a>,<a href="#id18">2</a>)</span></dt>
<dd><p>Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In <em>Advances in neural information processing systems</em>, 3391–3401. 2017.</p>
</dd>
<dt class="label" id="id74"><span class="brackets"><a class="fn-backref" href="#id19">BHB+18</a></span></dt>
<dd><p>Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, and others. Relational inductive biases, deep learning, and graph networks. <em>arXiv preprint arXiv:1806.01261</em>, 2018.</p>
</dd>
<dt class="label" id="id224"><span class="brackets">SchuttSK+18</span><span class="fn-backref">(<a href="#id20">1</a>,<a href="#id21">2</a>,<a href="#id23">3</a>)</span></dt>
<dd><p>Kristof T Schütt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R Müller. Schnet–a deep learning architecture for molecules and materials. <em>The Journal of Chemical Physics</em>, 148(24):241722, 2018.</p>
</dd>
<dt class="label" id="id225"><span class="brackets"><a class="fn-backref" href="#id22">CW22</a></span></dt>
<dd><p>Sam Cox and Andrew D White. Symmetric molecular dynamics. <em>arXiv preprint arXiv:2204.01114</em>, 2022.</p>
</dd>
<dt class="label" id="id80"><span class="brackets"><a class="fn-backref" href="#id24">ZSX+18</a></span></dt>
<dd><p>Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: gated attention networks for learning on large and spatiotemporal graphs. <em>arXiv preprint arXiv:1803.07294</em>, 2018.</p>
</dd>
<dt class="label" id="id91"><span class="brackets"><a class="fn-backref" href="#id25">HYL17</a></span></dt>
<dd><p>Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In <em>Advances in neural information processing systems</em>, 1024–1034. 2017.</p>
</dd>
<dt class="label" id="id93"><span class="brackets"><a class="fn-backref" href="#id26">EPBM19</a></span></dt>
<dd><p>Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. A fair comparison of graph neural networks for graph classification. In <em>International Conference on Learning Representations</em>. 2019.</p>
</dd>
<dt class="label" id="id92"><span class="brackets"><a class="fn-backref" href="#id26">SMBGunnemann18</a></span></dt>
<dd><p>Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. Pitfalls of graph neural network evaluation. <em>arXiv preprint arXiv:1811.05868</em>, 2018.</p>
</dd>
<dt class="label" id="id106"><span class="brackets">KGrossGunnemann20</span><span class="fn-backref">(<a href="#id28">1</a>,<a href="#id29">2</a>)</span></dt>
<dd><p>Johannes Klicpera, Janek Groß, and Stephan Günnemann. Directional message passing for molecular graphs. In <em>International Conference on Learning Representations</em>. 2020.</p>
</dd>
<dt class="label" id="id107"><span class="brackets"><a class="fn-backref" href="#id30">JES+20</a></span></dt>
<dd><p>Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael JL Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. <em>arXiv preprint arXiv:2009.01411</em>, 2020.</p>
</dd>
<dt class="label" id="id226"><span class="brackets"><a class="fn-backref" href="#id32">Velivckovic22</a></span></dt>
<dd><p>Petar Veličković. Message passing all the way up. <em>arXiv preprint arXiv:2202.11097</em>, 2022.</p>
</dd>
<dt class="label" id="id228"><span class="brackets"><a class="fn-backref" href="#id33">BFO+21</a></span></dt>
<dd><p>Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido F Montufar, and Michael Bronstein. Weisfeiler and lehman go cellular: cw networks. <em>Advances in Neural Information Processing Systems</em>, 34:2625–2640, 2021.</p>
</dd>
<dt class="label" id="id227"><span class="brackets"><a class="fn-backref" href="#id34">TZK21</a></span></dt>
<dd><p>Erik Thiede, Wenda Zhou, and Risi Kondor. Autobahn: automorphism-based graph neural nets. <em>Advances in Neural Information Processing Systems</em>, 2021.</p>
</dd>
</dl>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./dl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="layers.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">7. </span>Standard Layers</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="data.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9. </span>Input Data &amp; Equivariances</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Andrew D. White<br/>
    
        &copy; Copyright 2022.<br/>
      <div class="extra_footer">
        <a href="http://thewhitelab.org">thewhitelab.org</a> <div id="wh-modal"> <button class="wh-venti-button" aria-label="close modal" id="wh-modal-close">✕</button> <img id="wh-modal-img"> </div>
      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>